{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the arXiv ID for SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "datas = []\n",
    "unmatch = []\n",
    "# Regular expression pattern to find and capture the numerical parts\n",
    "pattern = r\"https://arxiv.org/pdf/(\\d{4}\\.\\d{5})\\.pdf\"\n",
    "\n",
    "\n",
    "with open('data/reason.md', 'r') as f:\n",
    "    for line in f.readlines():  \n",
    "        if 'https://arxiv' in line:\n",
    "            # Find all matches in the string\n",
    "            matches = re.findall(pattern, line)\n",
    "            datas.append(f'ARXIV:{matches[0]}')\n",
    "        elif 'pdf' in line:\n",
    "            unmatch.append(line)\n",
    "\n",
    "datas[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get TL;DR from SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "sch = SemanticScholar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_data(paper):\n",
    "    domains = ['title', 'authors', 'abstract', 'tldr', 'venue', 'referenceCount', 'citationCount', 'influentialCitationCount']\n",
    "\n",
    "    meta_data = {}\n",
    "    for domain in domains:\n",
    "        if domain in paper.keys():\n",
    "            if domain == 'authors':\n",
    "                authors = []\n",
    "                for author_domain in paper.__getattribute__(domain):\n",
    "                    authors.append(author_domain['name'])\n",
    "                meta_data['authors'] = authors\n",
    "            else:\n",
    "                domain_res = paper.__getattribute__(domain)\n",
    "                if domain == 'tldr':\n",
    "                    domain_res = str(domain_res)\n",
    "                meta_data[domain] = domain_res\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "meta_datas = []\n",
    "with open('result.json', 'a+') as fp:\n",
    "    for data in tqdm(datas):\n",
    "        try:\n",
    "            paper = sch.get_paper(data)\n",
    "            fp.writelines(get_meta_data(paper))\n",
    "        except:\n",
    "            fp.writelines({'Cannot find': data})\n",
    "        \n",
    "        meta_datas.append(get_meta_data(paper))\n",
    "        time.sleep(2) # hold 2 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arXiv searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "# -----\n",
    "# sort_by\n",
    "# Relevance \n",
    "# LastUpdatedDate \n",
    "# SubmittedDate\n",
    "# -----\n",
    "# order\n",
    "# Ascending\n",
    "# Descending\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"Large Language Models\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "  sort_order = arxiv.SortOrder.Descending,\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "for r in client.results(search):\n",
    "  print(r.title, r.published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "# Construct the default API client.\n",
    "client = arxiv.Client()\n",
    "\n",
    "# Search for the 10 most recent articles matching the keyword \"quantum.\"\n",
    "search = arxiv.Search(\n",
    "  query = \"Large Language Models\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "# `results` is a generator; you can iterate over its elements one by one...\n",
    "for r in client.results(search):\n",
    "  print(r.title)\n",
    "# ...or exhaust it into a list. Careful: this is slow for large results sets.\n",
    "all_results = list(results)\n",
    "print([r.title for r in all_results])\n",
    "\n",
    "# For advanced query syntax documentation, see the arXiv API User Manual:\n",
    "# https://arxiv.org/help/api/user-manual#query_details\n",
    "search = arxiv.Search(query = \"au:del_maestro AND ti:checkerboard\")\n",
    "first_result = next(client.results(search))\n",
    "print(first_result)\n",
    "\n",
    "# Search for the paper with ID \"1605.08386v1\"\n",
    "search_by_id = arxiv.Search(id_list=[\"1605.08386v1\"])\n",
    "# Reuse client to fetch the paper, then print its title.\n",
    "first_result = next(client.results(search))\n",
    "print(first_result.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_and_categorize_papers(venue_id):\n",
    "    url = f\"https://api2.openreview.net/notes?content.venueid={venue_id}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Initialize dictionaries to hold categorized papers\n",
    "    papers_by_type = {'Oral': [], 'Spotlight': [], 'Poster': []}\n",
    "    \n",
    "    # Iterate over all papers and categorize them\n",
    "    if 'notes' in data:\n",
    "        for note in data['notes']:\n",
    "            venue_info = note['content'].get('venue', {})\n",
    "            if 'value' in venue_info:\n",
    "                venue_value = venue_info['value']\n",
    "                if 'oral' in venue_value.lower():\n",
    "                    papers_by_type['Oral'].append(note)\n",
    "                elif 'spotlight' in venue_value.lower():\n",
    "                    papers_by_type['Spotlight'].append(note)\n",
    "                elif 'poster' in venue_value.lower():\n",
    "                    papers_by_type['Poster'].append(note)\n",
    "    \n",
    "    return papers_by_type\n",
    "\n",
    "# Usage of the function\n",
    "venue_id = \"ICML.cc/2024/Conference\"\n",
    "\n",
    "# venue_id = \"NeurIPS.cc/2023/Conference\"\n",
    "papers_by_type = fetch_and_categorize_papers(venue_id)\n",
    "\n",
    "# Print the results to verify\n",
    "# for paper_type, notes in papers_by_type.items():\n",
    "#     print(f\"\\n{paper_type} Papers:\")\n",
    "#     if notes:\n",
    "#         for note in notes[:5]:  # Limiting to first 5 papers for brevity\n",
    "#             title = note.get('content', {}).get('title', 'No title available')\n",
    "#             authors = \", \".join(note.get('content', {}).get('authors', []))\n",
    "#             abstract = note.get('content', {}).get('abstract', 'No abstract available')\n",
    "#             tldr = note.get('content', {}).get('TLDR', 'No TL;DR available')\n",
    "#             keywords = note.get('content', {}).get('keywords', 'No keywords available')\n",
    "#             link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "#             print(f\"Title: {title}\")\n",
    "#             print(f\"Authors: {authors}\")\n",
    "#             print(f\"Abstract: {abstract}\")\n",
    "#             print(f\"TL;DR: {tldr}\")\n",
    "#             print(f\"Keywords: {keywords}\")\n",
    "#             print(f\"Link: {link}\")\n",
    "#             print(\"---\")\n",
    "#     else:\n",
    "#         print(\"No papers found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper_type, notes in papers_by_type.items():\n",
    "    print(f\"\\n{paper_type} Papers:\")\n",
    "    if notes:\n",
    "        for note in notes: \n",
    "            title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "            authors = \", \".join(note.get('content', {}).get('authors', []).get('value'))\n",
    "            abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "            tldr = note.get('content', {}).get('TLDR', '')\n",
    "            if tldr:\n",
    "                tldr = tldr['value']\n",
    "            else:\n",
    "                tldr = 'No TL;DR available'\n",
    "\n",
    "            keywords = note.get('content', {}).get('keywords', 'No keywords available').get('value')\n",
    "            keywords = '; '.join(keywords)\n",
    "\n",
    "\n",
    "            if ('reasoning' in tldr.lower()) or ('reasoning' in keywords.lower()) or ('reasoning' in abstract.lower()) or ('reasoning' in title.lower()):\n",
    "                # print(f\"Reasoning Paper\")\n",
    "\n",
    "                # link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Authors: {authors}\")\n",
    "                # print(f\"Abstract: {abstract}\")\n",
    "                print(f\"TL;DR: {tldr}\")\n",
    "                print(f\"Keywords: {keywords}\")\n",
    "                # print(f\"Link: {link}\")\n",
    "                print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test the API endpoint directly and print a portion of the response\n",
    "url = \"https://api2.openreview.net/notes?content.venueid=ICLR.cc/2024/Conference\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Print the entire response to understand its structure\n",
    "print(json.dumps(data, indent=4))\n",
    "\n",
    "# Check if specific expected keys are present and print the first few entries if available\n",
    "if 'notes' in data and data['notes']:\n",
    "    for note in data['notes'][:5]:  # Print details of the first 5 entries\n",
    "        print(\"Title:\", note.get('content', {}).get('title', 'No title available'))\n",
    "        print(\"Authors:\", \", \".join(note.get('content', {}).get('authors', [])))\n",
    "        print(\"Abstract:\", note.get('content', {}).get('abstract', 'No abstract available'))\n",
    "        print(\"Link:\", f\"https://openreview.net/forum?id={note['id']}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"No notes found in the data.\")\n",
    "\n",
    "# import requests\n",
    "# import json\n",
    "\n",
    "# # Request a broad set of data to understand its structure\n",
    "# url = \"https://api2.openreview.net/notes?content.venueid=ICLR.cc/2024/Conference\"\n",
    "# response = requests.get(url)\n",
    "# data = response.json()\n",
    "\n",
    "# # Print a comprehensive view of the first few entries to inspect the structure\n",
    "# print(json.dumps(data['notes'][:5], indent=4))  # Print details of the first 5 entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Request a broad set of data to understand its structure\n",
    "url = \"https://api2.openreview.net/notes?content.venueid=NeurIPS.cc/2023/Conference\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Print a comprehensive view of the first few entries to inspect the structure\n",
    "print(json.dumps(data['notes'][:5], indent=4))  # Print details of the first 5 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.together.xyz/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"</s>\", \"[/INST]\"],\n",
    "    \"top_p\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful travel agent\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about San Francisco\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer 1940cc7aab91e20ee2f4d6d971dd8abe078db62293b29214485a956e193fb532\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
