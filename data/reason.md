# Foundation-Models-for-Reasoning

[toc]

## Scene
> specific
### Structural reasoning
> graph-related

[NeurIPS 2023] GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph.
[[paper]](https://arxiv.org/pdf/2309.13625.pdf)
[[code]](https://github.com/lixinustc/GraphAdapter)
> Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang.

[Arixv 2023.10] GraphGPT: Graph Instruction Tuning for Large Language Models.
[[paper]](https://arxiv.org/pdf/2310.13023.pdf)
[[code]](https://github.com/HKUDS/GraphGPT)
[[website]](https://graphgpt.github.io/)
> Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang.
 
[Arxiv 2023.10] Talk like a Graph: Encoding Graphs for Large Language Models.
[[paper]](https://arxiv.org/pdf/2310.04560.pdf)
> Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi.  

[Arxiv 2023/10] Beyond Text: A Deep Dive into Large Language Modelsâ€™ Ability on Understanding Graph Data.
[[paper]](https://arxiv.org/pdf/2310.04944.pdf)
> Yuntong Hu, Zheng Zhang, Liang Zhao

[Arxiv 2023/10] RelBERT: Embedding Relations with Language Models.
[[paper]](https://arxiv.org/pdf/2310.00299.pdf)
> Asahi Ushio, Jose Camacho-Collados, Steven Schockaert

[Arxiv 2023/10] Label-free Node Classification on Graphs with Large Language Models (LLMS).
[[paper]](https://arxiv.org/pdf/2310.04668.pdf)
> Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang

[Arxiv 2023/10] GRAPHLLM: BOOSTING GRAPH REASONING ABILITY OF LARGE LANGUAGE MODEL.
[[paper](https://arxiv.org/pdf/2310.05845.pdf)]
[[code](https://github.com/mistyreed63849/Graph-LLM)]
> Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang

[Arxiv 2023/10] (Node Classification) GraphText: Graph Reasoning in Text Space.
[[paper]](https://arxiv.org/pdf/2310.01089.pdf)
> Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein, Zhaocheng Zhu, Jian Tang

[ArXiv 2023/09] (Node Classification) Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why.
[[paper](https://arxiv.org/pdf/2309.16595.pdf)]
[[code](https://github.com/TRAIS-Lab/LLM-Structured-Data)]
> Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma

[Arxiv 2023/09] ChatGPT Informed Graph Neural Network for Stock Movement Prediction.
[[paper](https://arxiv.org/pdf/2306.03763.pdf)]
[[code](https://github.com/ZihanChen1995/ChatGPT-GNN-StockPredict)]
> Zihan Chen, Lei Nico Zheng, Cheng Lu, Jialu Yuan, Di Zhu

[Arxiv 2023/08] (Node Classification) Natural Language is All a Graph Needs.
[[paper]](https://arxiv.org/pdf/2308.07134.pdf)
> Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang

[Arxiv 2023/08]ðŸ”¥ (Node Classification) Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs
[[paper]](https://arxiv.org/pdf/2307.03393.pdf)
[[code]](https://github.com/CurryTang/Graph-LLM)
> Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang

[ArXiv 2023/07] (Graph Classification) Can Large Language Models Empower Molecular Property Prediction?
[[paper](https://arxiv.org/pdf/2307.07443.pdf)]
[[code](https://github.com/ChnQ/LLM4Mol)]
> Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu

[ArXiv 2023/06] (Graph Classification) GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning
[[paper](https://arxiv.org/pdf/2306.13089.pdf)]
[[code](https://github.com/zhao-ht/GIMLET)]
> Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu

[ICLR 2022] Node feature extraction by self-supervised multi-scale neighborhood prediction. 
[[paper]](https://arxiv.org/pdf/2111.00064.pdf)
[[code]](https://github.com/amzn/pecos/tree/mainline/examples/giant-xrt)
> Eli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Jiong Zhang, Olgica Milenkovic, Inderjit S Dhillon

[Arxiv 2023/05]ðŸ”¥ Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning.
[[paper]](https://arxiv.org/pdf/2305.19523.pdf)
[[code]](https://github.com/XiaoxinHe/TAPE)
> Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi

[ICLR 2024] GraphGPT: Graph Learning with Generative Pre-trained Transformers 
[[paper]](https://arxiv.org/pdf/2310.13023.pdf)

[ICLR 2024] LLM4GCL: CAN LARGE LANGUAGE MODEL EM-POWER GRAPH CONTRASTIVE LEARNING? [[paper]](https://openreview.net/pdf?id=wxClzZdjqP)

[KDD 2023] All in One: Multi-Task Prompting for Graph Neural Networks
[[paper]](https://arxiv.org/pdf/2307.01504.pdf) 
[[code]](https://github.com/sheldonresearch/ProG)
> Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan

[WWW 2023] Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer.
[[paper]](https://arxiv.org/pdf/2303.03922.pdf)
> Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing Xu, Wenting Song, Huajun Chen

[WWW 2023] GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks.
[[paper]](https://arxiv.org/pdf/2302.08043.pdf)
[[code]](https://github.com/zjukg/KGTransformer)

[SIGIR 2023] Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. 
[[paper]](https://arxiv.org/pdf/2210.10709.pdf)
[[code]](https://github.com/zjunlp/RAP)
> Zemin Liu, Xingtong Yu, Yuan Fang, Xinming Zhang

[NeurIPS 2023]ðŸ”¥ PRODIGY: Enabling In-context Learning Over Graphs. 
[[paper]](https://arxiv.org/pdf/2305.12600.pdf)
[[code]](https://github.com/snap-stanford/prodigy)
> Qian Huang, Hongyu Ren, Peng Chen, Gregor KrÅ¾manc, Daniel Zeng, Percy Liang, Jure Leskovec

[Arxiv 2023] A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges.
[[paper]](https://arxiv.org/pdf/2303.07275.pdf)
> Xuansheng Wu, Kaixiong Zhou, Mingchen Sun, Xin Wang, Ninghao Liu

[Arxiv 2023] Universal Prompt Tuning for Graph Neural Networks.
[[paper]](https://arxiv.org/pdf/2209.15240.pdf)
> Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, Lei Chen

[Arxiv 2023] SGL-PT: A Strong Graph Learner with Graph Prompt Tuning.
[[paper]](https://arxiv.org/pdf/2302.12449.pdf)
> Yun Zhu, Jianhao Guo, Siliang Tang

[ArXiv 2023] CodeKGC: Code Language Model for Generative Knowledge Graph Construction 
[[paper](https://arxiv.org/pdf/2304.09048.pdf)]
[[code](https://github.com/zjunlp/DeepKE/tree/main/example/llm/CodeKGC)]
> Zhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, Ningyu Zhang

[ArXiv 2023] StructGPT: A General Framework for Large Language Model to Reason over Structured Data. 
[[paper](https://arxiv.org/pdf/2305.09645.pdf)]
[[code](https://github.com/RUCAIBox/StructGPT)]
> Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen

[ArXiv 2023] PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs. 
[[paper](https://arxiv.org/pdf/2305.12392.pdf)]
[[code](https://github.com/Jiuzhouh/PiVe)]
> Jiuzhou Han, Nigel Collier, Wray Buntine, Ehsan Shareghi

[Arxiv 2023/10] An Investigation of LLMsâ€™ Inefficacy in Understanding Converse Relations.
[[paper]](https://arxiv.org/pdf/2310.05163.pdf)
[[code]](https://github.com/3B-Group/ConvRe)
> Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili

[Arxiv 2023/10] Integrating Graphs with Large Language Models: Methods and Prospects.
[[paper]](https://arxiv.org/pdf/2310.05499.pdf)
> Shirui Pan, Yizhen Zheng, Yixin Liu

[Arxiv 2023/07] GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking.
[[paper]](https://arxiv.org/pdf/2305.15066.pdf)
[[code]](https://github.com/SpaceLearner/Graph-GPT)
> Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han

[ICLR 2024] Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models 
[[paper]](https://arxiv.org/pdf/2310.01290.pdf) 
[[code]](https://github.com/Wenwen-D/KnowledgeCrosswords)
> Wenxuan Ding, Shangbin Feng, Yuhan Liu, Zhaoxuan Tan, Vidhisha Balachandran, Tianxing He, Yulia Tsvetkov

[Arxiv 2023/06] Unifying Large Language Models and Knowledge Graphs: A Roadmap. 
[[paper]](https://arxiv.org/pdf/2306.08302.pdf)
> Shirui Pan, Senior Member, IEEE, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu

[Arxiv 2022/10] Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing.
[[paper]](https://arxiv.org/pdf/2212.10789.pdf)
> Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar

[ICLR 2023] Learning on large-scale text-attributed graphs via variational inference.
[[paper]](https://arxiv.org/pdf/2210.14709.pdf)
[[code]](https://github.com/AndyJZhao/GLEM)
> Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang


### Visual reasoning

[NeurIPS 2023] Active Reasoning in an Open-World Environment
[[paper]](https://arxiv.org/pdf/2311.02018.pdf)
[[code]](https://sites.google.com/view/conan-active-reasoning)
> Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu

[NeurIPS 2023] Large Language Models are Visual Reasoning Coordinators
[[paper]](https://arxiv.org/pdf/2310.15166.pdf)
[[code]](https://github.com/cliangyu/Cola)
> Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu

[NeurIPS 2023] Interactive Visual Reasoning under Uncertainty
[[paper]](https://arxiv.org/pdf/2206.09203.pdf)
[[code]](https://github.com/mjtsu/IVRE)
> Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, Yixin Zhu

[NeurIPS 2023] Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning
[[paper]](https://arxiv.org/pdf/2311.17365.pdf)
[[code]](https://github.com/enlighten0707/Symbol-LLM)
> Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, Cewu Lu

[NeurIPS 2023] VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use
[[paper]](https://arxiv.org/pdf/2308.06595.pdf)
[[code]](https://visit-bench.github.io/)
> Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zh, Anas Awadalla, Josh Gardner, Rohan Taori, Ludwig Schimdt

### Moral reasoning
> Focuses on the reasoning process of what is good and what is evil and how people make choices in ethical decision-making.

[NeurIPS 2023] LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models
[[paper]](https://arxiv.org/pdf/2308.11462.pdf)
> Neel Guha, Julian Nyarko, Daniel Ho, Christopher RÃ©, Adam Chilton, Aditya K, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, Dmitry Talisman, Enam Hoque, Faiz Surani, Frank Fagan, Galit Sarfaty, Gregory Dickinson, Haggai Porat, Jason Hegland, Jessica Wu, Joe Nudell, Joel Niklaus, John Nay, Jonathan Choi, Kevin Tobia, Margaret Hagan, Megan Ma, Michael Livermore, Nikon Rasumov-Rahe, Nils Holzenberger, Noam Kolt, Peter Henderson, Sean Rehaag, Sharad Goel, Shang Gao, Spencer Williams, Sunny Gandhi, Tom Zur, Varun Iyer, Zehua Li

### Social reasoning
> the cognitive process of making judgments and decisions in social situations based on the interpretation of social cues and information about others' mental states and behaviors.

### Math reasoning
[Arxiv 2110] Training Verifiers to Solve Math Word Problems 
[[paper]](https://arxiv.org/pdf/2110.14168.pdf) 
[[code]](https://github.com/openai/grade-school-math?tab=readme-ov-file)
> Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman

[Arxiv 2023/10] Mathvista: Evaluating math reasoning in visual contexts with gpt-4v, bard, and other large multimodal models. 
[[paper]](https://arxiv.org/pdf/2310.02255.pdf) 
[[code]](https://github.com/lupantech/MathVista)
> Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao

[Arxiv 2211]Teaching Algorithmic Reasoning via In-context Learning 
[[paper]](https://arxiv.org/pdf/2211.09066.pdf)
> Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, Hanie Sedghi

### Creative reasoning
[Arxiv 2310] MacGyver: Are Large Language Models Creative Problem Solvers? [[paper]](https://arxiv.org/pdf/2311.09682.pdf)
> Yufei Tian,  Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman

## Method
> general
### Logical reasoning
#### Deductive reasoning
> Given a set of rules, the result is obtained by reasoning from the rules e.g. [Arixv 2023.01] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

[Arixv 2023.10] PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization. 
[[paper]](https://arxiv.org/pdf/2310.16427.pdf)
> Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu.

[Arixv 2023.10] Tree Prompting: Efficient Task Adaptation without Fine-Tuning. [[paper]](https://arxiv.org/pdf/2310.14034.pdf)
[[code]](https://github.com/csinva/tree-prompt)
> John X. Morris, Chandan Singh, Alexander M. Rush, Jianfeng Gao, Yuntian Deng.

[Arixv 2023.01] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. [[paper]](https://arxiv.org/pdf/2201.11903.pdf)
> Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou

[NeurIPS 2023] Why think step by step? Reasoning emerges from the locality of experience [[paper]](https://arxiv.org/pdf/2304.03843.pdf)
> Ben Prystawski, Michael Y. Li, Noah D. Goodman

[Arixv 2023.05] Tree of Thoughts: Deliberate Problem Solving with Large Language Models. 
[[paper]](https://arxiv.org/pdf/2305.10601.pdf) 
[[code]](https://github.com/ysymyth/tree-of-thought-llm)
> Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan

[Arixv 2023.08] Graph of Thoughts: Solving Elaborate Problems with Large Language Models.  [[paper]](https://arxiv.org/pdf/2308.09687.pdf) 
> Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler

[ICLR 2024] Chain-of-Experts: When LLMs Meet Complex Operations Research Problems  [[paper]](https://openreview.net/pdf?id=HobyL1B9CZ)

[ICLR 2024] Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding [[paper]](https://openreview.net/pdf?id=4L0xnS4GQM)

[ICLR 2024] Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models [[paper]](https://openreview.net/pdf?id=qBL04XXex6)

[NeurIPS 2023] WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding. [[paper]](https://openreview.net/pdf?id=ZrG8kTbt70)
> Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, Carl Yang.

[ArXiv 2023.10] Towards Foundation Models for Knowledge Graph Reasoning. [[paper]](https://arxiv.org/pdf/2310.04562.pdf)
> Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, Zhaocheng Zhu.

[ArXiv 2023.10] Large Language Models can Learn Rules.
[[paper]](https://arxiv.org/pdf/2310.07064.pdf)
> Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai.

[ICLR 2024] Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning [[paper]](https://openreview.net/pdf?id=ZGNWW7xZ6Q)
> Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan

[arXiv 2023/10] Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples.  [[paper]](https://arxiv.org/pdf/2305.15269.pdf)
> Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He

[NeurIPS 2023] SPRING: Studying the Paper and Reasoning to Play Games. 
[[paper]](https://arxiv.org/pdf/2305.15486.pdf) 
[[code]](https://github.com/Holmeswww/SPRING)
> Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Tom Mitchell, Yuanzhi Li

[NeurIPS 2023] Parsel: Algorithmic Reasoning with Language Models by Composing Decompositions 
[[paper]](https://arxiv.org/pdf/2212.10561.pdf) 
[[code]](https://github.com/ezelikman/parsel)
> Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber

[NeurIPS 2023] DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models 
[[paper]](https://arxiv.org/pdf/2310.16436.pdf) 
[[code]](https://github.com/SooLab/DDCOT)
 > Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, Sibei Yang

[NeurIPS 2023] Deductive Verification of Chain-of-Thought Reasoning 
[[paper]](https://arxiv.org/pdf/2306.03872.pdf) 
[[code]](https://github.com/lz1oceani/verify_cot)
> Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su

[NeurIPS 2023] PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change 
[[paper]](https://arxiv.org/pdf/2206.10498.pdf)
> Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati

[NeurIPS 2023] Self-Evaluation Guided Beam Search for Reasoning 
[[paper]](https://arxiv.org/pdf/2305.00633.pdf) 
[[coede]](https://guideddecoding.github.io/)
> Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, Qizhe Xie

[NeurIPS 2023] Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[paper]](https://arxiv.org/pdf/2304.09842.pdf) [[code]](https://github.com/lupantech/chameleon-llm)
> Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao

[Arxiv 2205] Large Language Models are Zero-Shot Reasoners [[paper]](https://arxiv.org/pdf/2205.11916.pdf) [[code]](https://github.com/kojima-takeshi188/zero_shot_cot)
> Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa

[Arxiv 2209] Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models [[paper]](https://arxiv.org/pdf/2209.08141.pdf)
> Ben Prystawski, Paul Thibodeau, Christopher Potts, Noah D. Goodman

[Arxiv 2210] Language Models are Multilingual Chain-of-Thought Reasoners [[paper]](https://arxiv.org/pdf/2210.03057.pdf) [[code]](https://github.com/google-research/url-nlp)
> Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei

[Arxiv 2210] Large Language Models are few(1)-shot Table Reasoners [[paper]](https://arxiv.org/pdf/2210.06710.pdf) [[code]](https://github.com/wenhuchen/TableCoT)
> Wenhu Chen

[Arxiv 2210] Language Models of Code are Few-Shot Commonsense Learners [[paper]](https://arxiv.org/pdf/2210.07128.pdf) [[code]](https://github.com/reasoning-machines/CoCoGen)
> Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig

[Arxiv 2211] PAL: Program-aided Language Models [[paper]](https://arxiv.org/pdf/2211.10435.pdf) [[code]](https://github.com/reasoning-machines/pal)
> Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig

[Arxiv 2211] Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks [[paper]](https://arxiv.org/pdf/2211.12588.pdf) [[code]](https://github.com/wenhuchen/Program-of-Thoughts)
> Wenhu Chen, Xueguang Ma, Xinyi Wang, William W. Cohen

[Arxiv 2301] Rethinking with Retrieval: Faithful Large Language Model Inference [[paper]](https://arxiv.org/pdf/2301.00303.pdf) [[code]](https://github.com/HornHehhf/RR)
> Hangfeng He, Hongming Zhang, Dan Roth

[Arxiv 2203] Self-Consistency Improves Chain of Thought Reasoning in Language Models [[paper]](https://arxiv.org/pdf/2203.11171.pdf) [[code]](https://github.com/lastmile-ai/aiconfig/tree/main/cookbooks/Multi-LLM-Consistency)
> Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou

[Arxiv 2206] Making Large Language Models Better Reasoners with Step-Aware Verifier
[[paper]](https://arxiv.org/pdf/2206.02336.pdf)
> Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen

[Arxiv 2210] Automatic Chain of Thought Prompting in Large Language Models [[paper]](https://arxiv.org/pdf/2210.03493.pdf) [[code]](https://github.com/amazon-science/auto-cot)
> Zhuosheng Zhang, Aston Zhang, Mu Li, Alex 

[Arxiv 2210] Complexity-Based Prompting for Multi-Step Reasoning [[paper]](https://arxiv.org/pdf/2210.00720.pdf) [[code]](https://github.com/FranxYao/Complexity-Based-Prompting?tab=readme-ov-file)
> Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot

[Arxiv 2211] Automatic Engineering of Long Prompts [[paper]](https://arxiv.org/pdf/2311.10117.pdf)
> Cho-Jui Hsieh, Si Si, Felix X. Yu, Inderjit S. Dhillon

[Arxiv 2212] Large Language Models are Better Reasoners with Self-Verification [[paper]](https://arxiv.org/pdf/2212.09561.pdf) [[code]](https://github.com/WENGSYX/Self-Verification)
> Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao

[Arxiv 2205] Least-to-Most Prompting Enables Complex Reasoning in Large Language Models [[paper]](https://arxiv.org/pdf/2205.10625.pdf) 
> Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, Ed Chi

[Arxiv 2209] Compositional Semantic Parsing with Large Language Models [[paper]](https://arxiv.org/pdf/2209.15003.pdf)
> Andrew Drozdov, Nathanael SchÃ¤rli, Ekin AkyÃ¼rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou

[Arxiv 2210] Decomposed Prompting: A Modular Approach for Solving Complex Tasks [[paper]](https://arxiv.org/pdf/2210.02406.pdf) [[code]](https://github.com/allenai/DecomP)
> Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal

[Arxiv 2210] Measuring and Narrowing the Compositionality Gap in Language Models [[paper]](https://arxiv.org/pdf/2210.03350.pdf) [[code]](https://github.com/ofirpress/self-ask)
> Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, Mike Lewis

[Arxiv 2212] Successive Prompting for Decomposing Complex Questions [[paper]](https://arxiv.org/pdf/2212.04092.pdf)
> Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner

[Arxiv 2301] Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning [[paper]](https://arxiv.org/pdf/2301.13808.pdf)
> Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li

[Arxiv 2205] Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning [[paper]](https://arxiv.org/pdf/2205.09712.pdf)
> Antonia Creswell, Murray Shanahan, Irina Higgins

#### Inductive reasoning
> Given some information, summarise the generalised rules. e.g. [ArXiv 2023.10] Large Language Models can Learn Rules.

[ArXiv 2023.10] Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement.
[[paper]](https://arxiv.org/pdf/2310.08559.pdf)
> Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren.

[ArXiv 2023.10] Large Language Models can Learn Rules.
[[paper]](https://arxiv.org/pdf/2310.07064.pdf)
> Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans, Hanjun Dai.

[ArXiv 2023.10] ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning.
[[paper]](https://arxiv.org/pdf/2309.01538.pdf) 
> Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan.

[Arxiv 2023/10] Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models.
[[paper]](https://arxiv.org/pdf/2310.01074.pdf)
> Chenhan Yuan, Qianqian Xie, Jimin Huang, Sophia Ananiadou

[Arxiv 2023.10] MINPROMPT: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering.
[[paper]](https://arxiv.org/pdf/2310.05007.pdf)
> Xiusi Chen, Jyun-Yu Jiang, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-Fu Yu, Wei Wang

[IEEE 2023.06] Unifying Structure Reasoning and Language Pre-training for Complex Reasoning Tasks.
[[paper]](https://arxiv.org/pdf/2301.08913.pdf)
> Siyuan Wang, Zhongyu Wei, Jiarong Xu, Taishan Li, Zhihao Fan.

[ICLR 2024] Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-Temporal Reasoning 
[[paper]](https://openreview.net/pdf?id=fe8CzLTMG1)
> Mohamed Aghzal, Erion Plaku, Ziyu Yao

[ICLR 2024] Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph 
[[paper]](https://openreview.net/pdf?id=nnVO1PvbTv)
[[code]](https://github.com/IDEA-FinAI/ToG)
> Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, Jian Guo

[NeurIPS 2023] Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models
[[paper]](https://arxiv.org/pdf/2305.19595.pdf)
> Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, Shimon Ullman, Leonid Karlinsky

[NeurIPS 2023] SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning
[[paper]](https://arxiv.org/pdf/2306.12552.pdf)
[[code]](https://github.com/yunx-z/situated_gen)
> Yunxiang Zhang, Xiaojun Wan

[NeurIPS 2023] RECKONING: Reasoning through Dynamic Knowledge Encoding
[[paper]](https://arxiv.org/pdf/2305.06349.pdf)
> Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut

[NeurIPS 2023] Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks
[[paper]](https://arxiv.org/pdf/2305.18395.pdf)
[[code]](https://github.com/Nardien/KARD)
> Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, Sung Ju Hwang

#### Abductive reasoning
> Reason the most likely explanation from some evidence or phenomenon observed, but with exceptions. e.g. [ICLR 2024] Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs

[ICLR 2024] Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs 
[[paper]](https://openreview.net/forum?id=gjeQKFxFpZ)
> Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi

[AAAI 2022] Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs. 
[[paper](https://ojs.aaai.org/index.php/AAAI/article/view/21286)]
> Berkeley R Andrus, Yeganeh Nasiri, Shilong Cui, Benjamin Cullen, Nancy Fulda

[NeurIPS 2023] Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning
[[paper]](https://arxiv.org/pdf/2305.16646.pdf)
> Xiaoming Shi, Siqiao Xue, Kangrui Wang, Fan Zhou, James Y. Zhang, Jun Zhou, Chenhao Tan, Hongyuan Mei

[NeurIPS 2023] BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information
[[paper]](https://arxiv.org/pdf/2306.07934.pdf)
> Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, Deepak Ramachandran


### Causal reasoning

[NeurIPS 2023] CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models.
[[paper]](https://arxiv.org/pdf/2312.04350.pdf)
[[code]](https://github.com/causalNLP/cladder)
> Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard SchÃ¶lkopf

[NeurIPS 2023] TART: A plug-and-play Transformer module for task-agnostic reasoning
[[paper]](https://arxiv.org/pdf/2306.07536.pdf)
[[code]](https://github.com/HazyResearch/TART)
> Kush Bhatia, Avanika Narayan, Christopher De Sa, Christopher RÃ©

[Arxiv 24/01] A Comprehensive Study of Knowledge Editing for Large Language Models
[[paper]](https://arxiv.org/pdf/2401.01286.pdf)
[[code]](https://github.com/zjunlp/EasyEdit)
> Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen

### Theory-of-mind
> Analysing people's behaviour and reasoning about their intentions and mental states

[Arxiv 2023/10] How FaR Are Large Language Models From Agents with Theory-of-Mind? 
[[paper]](https://arxiv.org/pdf/2310.03051.pdf)
> Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, Manaal Faruqui

[ACL 2023] Minding Language Modelsâ€™ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker. 
[[paper]](https://aclanthology.org/2023.acl-long.780.pdf)
> Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov

[Arxiv 2023/04] Boosting Theory-of-Mind Performance in Large Language Models via Prompting. 
[[paper]](https://arxiv.org/pdf/2304.11490.pdf)
> Shima Rahimi Moghaddam, Christopher J. Honey

[ICLR 2022] ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind. [[paper]](https://arxiv.org/pdf/2111.09189.pdf)
> Yuanfei Wang, Fangwei Zhong, Jing Xu, Yizhou Wang

[NeurIPS 2023] Understanding Social Reasoning in Language Models with Language Models
[[paper]](https://arxiv.org/pdf/2306.15448.pdf)
> Kanishk Gandhi, Jan-Philipp FrÃ¤nken, Tobias Gerstenberg, Noah D. Goodman




## Survey & others

### Survey
[ACL 2023 | Survey] Towards Reasoning in Large Language Models: A Survey [[paper]](https://aclanthology.org/2023.findings-acl.67.pdf)
> Jie Huang, Kevin Chen-Chuan Chang

[Arxiv 2206] Emergent Abilities of Large Language Models
[[paper]](https://arxiv.org/pdf/2206.07682.pdf)
> Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus

[Arxiv 2207] Language Model Cascades
[[paper]](https://arxiv.org/pdf/2207.10342.pdf)
> David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton

[Arxiv 2212] Reasoning with Language Model Prompting: A Survey
[[paper]](https://arxiv.org/pdf/2212.09597.pdf)
> Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen

[Arxiv 2212] A Survey of Deep Learning for Mathematical Reasoning
[[paper]](https://arxiv.org/pdf/2212.10535.pdf)
> Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang

[Arxiv 2212] A Survey on In-context Learning
[[paper]](https://arxiv.org/pdf/2301.00234.pdf)
> Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui

[Arxiv 2303] Natural Language Reasoning, A Survey
[[paper]](https://arxiv.org/pdf/2303.14725.pdf)
> Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang

[Arxiv 2310] Towards Better Chain-of-Thought Prompting Strategies: A Survey
[[paper]](https://arxiv.org/pdf/2310.04959.pdf)
> Zihan Yu,  Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen

[Arxiv 2310] A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
[[paper]](https://arxiv.org/pdf/2309.15402.pdf)
> Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He1, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu

### Others
> fine-tuning
[Arxiv 2312] NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation
[[paper]](https://arxiv.org/pdf/2312.05979.pdf)
> Peter West, Ronan Le Bras, Taylor Sorensen, Bill Yuchen Lin, Liwei Jiang, Ximing Lu, Khyathi Chandu, Jack Hessel, Ashutosh Baheti, Chandra Bhagavatula, Yejin Choi

> multi-reasoning
[EMNLP 2023] Getting MoRE out of Mixture of Language Model Reasoning Experts
[[paper]](https://aclanthology.org/2023.findings-emnlp.552.pdf)
> Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan Boyd-Graber

[Arxiv 2201] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents [[paper]](https://arxiv.org/pdf/2201.07207.pdf) [[code]](https://github.com/huangwl18/language-planner)
> Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch

## Other resources
- [XiaoxinHe/Awesome-Graph-LLM](https://github.com/XiaoxinHe/Awesome-Graph-LLM)
- [RManLuo/Awesome-LLM-KG](https://github.com/RManLuo/Awesome-LLM-KG)
- [zjukg/KG-LLM-Papers](https://github.com/zjukg/KG-LLM-Papers)
- [Zhihu LLM+KG papers](https://zhuanlan.zhihu.com/p/641746485)
- [Prompt Engineering Guide](https://learnprompting.org/docs/basics)
- [Prompt Engineering](https://www.promptingguide.ai/)
- [PromptPapers](https://github.com/thunlp/PromptPapers)
- [CoT Papers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)
- [Knowledge Graph Embedding](https://github.com/xinguoxia/KGE)