[
    {
        "title": "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph",
        "authors": [
            "Xin Li",
            "Dongze Lian",
            "Zhihe Lu",
            "Jiawang Bai",
            "Zhibo Chen",
            "Xinchao Wang"
        ],
        "abstract": "Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapter-based methods. The code will be released at https://github.com/lixinustc/GraphAdapter",
        "tldr": "An effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge with a dual knowledge graph, which significantly outperforms previous adapter-based methods.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 81,
        "citationCount": 6,
        "influentialCitationCount": 0
    },
    {
        "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
        "authors": [
            "Jiabin Tang",
            "Yuhao Yang",
            "Wei Wei",
            "Lei Shi",
            "Lixin Su",
            "Suqi Cheng",
            "Dawei Yin",
            "Chao Huang"
        ],
        "abstract": "Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.",
        "tldr": "The GraphGPT framework is presented, which combines a text-graph grounding component to establish a connection between textual information and graph structures, and a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector, that explores self-supervised graph structural signals and task-specific graph instructions.",
        "venue": "arXiv.org",
        "referenceCount": 58,
        "citationCount": 20,
        "influentialCitationCount": 1
    },
    {
        "title": "Talk like a Graph: Encoding Graphs for Large Language Models",
        "authors": [
            "Bahare Fatemi",
            "Jonathan Halcrow",
            "Bryan Perozzi"
        ],
        "abstract": "Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "tldr": "This work performs the first comprehensive study of encoding graph-structured data as text for consumption by LLMs, and shows how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.",
        "venue": "arXiv.org",
        "referenceCount": 48,
        "citationCount": 14,
        "influentialCitationCount": 4
    },
    {
        "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data",
        "authors": [
            "Yuntong Hu",
            "Zhengwu Zhang",
            "Liang Zhao"
        ],
        "abstract": "Large language models (LLMs) have achieved impressive performance on many natural language processing tasks. However, their capabilities on graph-structured data remain relatively unexplored. In this paper, we conduct a series of experiments benchmarking leading LLMs on diverse graph prediction tasks spanning node, edge, and graph levels. We aim to assess whether LLMs can effectively process graph data and leverage topological structures to enhance performance, compared to specialized graph neural networks. Through varied prompt formatting and task/dataset selection, we analyze how well LLMs can interpret and utilize graph structures. By comparing LLMs' performance with specialized graph models, we offer insights into the strengths and limitations of employing LLMs for graph analytics. Our findings provide insights into LLMs' capabilities and suggest avenues for further exploration in applying them to graph analytics.",
        "tldr": "Estimating whether LLMs can effectively process graph data and leverage topological structures to enhance performance, compared to specialized graph neural networks and offering insights into the strengths and limitations of employing LLMs for graph analytics is offered.",
        "venue": "arXiv.org",
        "referenceCount": 48,
        "citationCount": 4,
        "influentialCitationCount": 1
    },
    {
        "title": "RelBERT: Embedding Relations with Language Models",
        "authors": [
            "Asahi Ushio",
            "Jos\u00e9 Camacho-Collados",
            "Steven Schockaert"
        ],
        "abstract": "Many applications need access to background knowledge about how different concepts and entities are related. Although Knowledge Graphs (KG) and Large Language Models (LLM) can address this need to some extent, KGs are inevitably incomplete and their relational schema is often too coarse-grained, while LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.",
        "tldr": "This work proposes to extract relation embeddings from relatively small language models to capture relational similarity in a surprisingly fine-grained way, and finds that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger.",
        "venue": "arXiv.org",
        "referenceCount": 143,
        "citationCount": 0,
        "influentialCitationCount": 0
    },
    {
        "title": "Label-free Node Classification on Graphs with Large Language Models (LLMS)",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hongzhi Wen",
            "Haoyu Han",
            "Wei-dong Jin",
            "Haiyang Zhang",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "abstract": "In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to obtain annotations of high quality, representativeness, and diversity, thereby enhancing GNN performance with less cost? To tackle this challenge, we develop an annotation quality heuristic and leverage the confidence scores derived from LLMs to advanced node selection. Comprehensive experimental results validate the effectiveness of LLM-GNN. In particular, LLM-GNN can achieve an accuracy of 74.9% on a vast-scale dataset \\products with a cost less than 1 dollar.",
        "tldr": "This work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN, which amalgamates the strengths of both GNNs and LLMs while mitigating their limitations while leveraging the confidence scores derived from LLMs to advanced node selection.",
        "venue": "arXiv.org",
        "referenceCount": 59,
        "citationCount": 12,
        "influentialCitationCount": 0
    },
    {
        "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
        "authors": [
            "Ziwei Chai",
            "Tianjie Zhang",
            "Liang Wu",
            "Kaiqiao Han",
            "Xiaohai Hu",
            "Xuanwen Huang",
            "Yang Yang"
        ],
        "abstract": "The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks.",
        "tldr": "GraphLLM is introduced, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs and equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power ofgraph learning models.",
        "venue": "arXiv.org",
        "referenceCount": 38,
        "citationCount": 13,
        "influentialCitationCount": 2
    },
    {
        "Cannot_Find": "ARXIV:2310.01089"
    },
    {
        "title": "Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?",
        "authors": [
            "Jin Huang",
            "Xingjian Zhang",
            "Qiaozhu Mei",
            "Jiaqi Ma"
        ],
        "abstract": "Large language models (LLMs) are gaining increasing attention for their capability to process graphs with rich text attributes, especially in a zero-shot fashion. Recent studies demonstrate that LLMs obtain decent text classification performance on common text-rich graph benchmarks, and the performance can be improved by appending encoded structural information as natural languages into prompts. We aim to understand why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs. First, we rule out the concern of data leakage by curating a novel leakage-free dataset and conducting a comparative analysis alongside a previously widely-used dataset. Second, as past work usually encodes the ego-graph by describing the graph structure in natural language, we ask the question: do LLMs understand the graph structure in accordance with the intent of the prompt designers? Third, we investigate why LLMs can improve their performance after incorporating structural information. Our exploration of these questions reveals that (i) there is no substantial evidence that the performance of LLMs is significantly attributed to data leakage; (ii) instead of understanding prompts as graph structures as intended by the prompt designers, LLMs tend to process prompts more as contextual paragraphs and (iii) the most efficient elements of the local neighborhood included in the prompt are phrases that are pertinent to the node label, rather than the graph structure.",
        "tldr": "There is no substantial evidence that the performance of LLMs is significantly attributed to data leakage, and instead of understanding prompts as graph structures as intended by the prompt designers, LLMs tend to process prompts more as contextual paragraphs.",
        "venue": "",
        "referenceCount": 45,
        "citationCount": 0,
        "influentialCitationCount": 0
    },
    {
        "title": "ChatGPT Informed Graph Neural Network for Stock Movement Prediction",
        "authors": [
            "Zihan Chen",
            "Lei Zheng",
            "Chengyu Lu",
            "Jialu Yuan",
            "Dixian Zhu"
        ],
        "abstract": "ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for inferring dynamic network structures from temporal textual data, specifically financial news, remains an unexplored frontier. In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN). Our framework adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks. The experimental results from stock movement forecasting indicate our model has consistently outperformed the state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown. This superior performance highlights the potential of ChatGPT for text-based network inferences and underscores its promising implications for the financial sector.",
        "tldr": "A novel framework is introduced that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN) and adeptly extracts evolving network structures from textual data, and incorporates these networks into graph neural networks for subsequent predictive tasks.",
        "venue": "Social Science Research Network",
        "referenceCount": 33,
        "citationCount": 14,
        "influentialCitationCount": 1
    },
    {
        "title": "Language is All a Graph Needs",
        "authors": [
            "Ruosong Ye",
            "Caiqi Zhang",
            "Runhui Wang",
            "Shuyuan Xu",
            "Yongfeng Zhang"
        ],
        "abstract": "The emergence of large-scale pre-trained language models has revolutionized various AI research domains. Transformers-based Large Language Models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with independent data samples such as images, videos or texts, graphs usually contain rich structural and relational information. Meanwhile, language, especially natural language, being one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph problems into the generative language modeling framework remains very limited. Considering the rising prominence of LLMs, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model) with highly scalable prompts based on natural language instructions. We use natural language to describe multi-scale geometric structure of the graph and then instruction finetune an LLM to perform graph tasks, which enables Generative Graph Learning. Our method surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning. Our code is open-sourced at https://github.com/agiresearch/InstructGLM.",
        "tldr": "This paper proposes InstructGLM (Instruction-finetuned Graph Language Model), a method that surpasses all GNN baselines on ogbn-arxiv, Cora and PubMed datasets, underscoring its effectiveness and sheds light on generative LLMs as new foundation model for graph machine learning.",
        "venue": "",
        "referenceCount": 104,
        "citationCount": 45,
        "influentialCitationCount": 5
    },
    {
        "title": "Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs",
        "authors": [
            "Zhikai Chen",
            "Haitao Mao",
            "Hang Li",
            "Wei Jin",
            "Haifang Wen",
            "Xiaochi Wei",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Wenqi Fan",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at https://github.com/CurryTang/Graph-LLM.",
        "tldr": "This paper aims to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigates two possible pipelines: LLMs-as-Enhancers and LLMs -as-Predictors.",
        "venue": "arXiv.org",
        "referenceCount": 109,
        "citationCount": 66,
        "influentialCitationCount": 14
    },
    {
        "title": "Can Large Language Models Empower Molecular Property Prediction?",
        "authors": [
            "Chen Qian",
            "Huayi Tang",
            "Zhi-Jiang Yang",
            "Hongsi Liang",
            "Y. Liu"
        ],
        "abstract": "Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \\url{https://github.com/ChnQ/LLM4Mol},.",
        "tldr": "This work employs LLMs to generate semantically enriched explanations for the original SMILES and then uses that to fine-tune a small-scale LM model for multiple downstream tasks and highlights the superiority of text explanations as molecular representations across multiple benchmark datasets.",
        "venue": "arXiv.org",
        "referenceCount": 40,
        "citationCount": 15,
        "influentialCitationCount": 0
    },
    {
        "title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
        "authors": [
            "Haiteng Zhao",
            "Shengchao Liu",
            "Chang Ma",
            "Hannan Xu",
            "Jie Fu",
            "Zhihong Deng",
            "Lingpeng Kong",
            "Qi Liu"
        ],
        "abstract": "Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv.1",
        "tldr": "Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv.",
        "venue": "bioRxiv",
        "referenceCount": 89,
        "citationCount": 15,
        "influentialCitationCount": 2
    },
    {
        "title": "Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction",
        "authors": [
            "Eli Chien",
            "Wei-Cheng Chang",
            "Cho-Jui Hsieh",
            "Hsiang-Fu Yu",
            "Jiong Zhang",
            "O. Milenkovic",
            "I. Dhillon"
        ],
        "abstract": "Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take numerical node features and graph structure as inputs, have been shown to achieve state-of-the-art performance on various graph-related learning tasks. Recent works exploring the correlation between numerical node features and graph structure via self-supervised learning have paved the way for further performance improvements of GNNs. However, methods used for extracting numerical node features from raw data are still graph-agnostic within standard GNN pipelines. This practice is sub-optimal as it prevents one from fully utilizing potential correlations between graph topology and node attributes. To mitigate this issue, we propose a new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT). GIANT makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets. We also provide a theoretical analysis that justifies the use of XMC over link prediction and motivates integrating XR-Transformers, a powerful method for solving XMC problems, into the GIANT framework. We demonstrate the superior performance of GIANT over the standard GNN pipeline on Open Graph Benchmark datasets: For example, we improve the accuracy of the top-ranked method GAMLP from $68.25\\%$ to $69.67\\%$, SGC from $63.29\\%$ to $66.10\\%$ and MLP from $47.24\\%$ to $61.10\\%$ on the ogbn-papers100M dataset by leveraging GIANT.",
        "tldr": "A new self-supervised learning framework, Graph Information Aided Node feature exTraction (GIANT), which makes use of the eXtreme Multi-label Classification (XMC) formalism, which is crucial for fine-tuning the language model based on graph information, and scales to large datasets.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 70,
        "citationCount": 72,
        "influentialCitationCount": 15
    },
    {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
        "authors": [
            "Xiaoxin He",
            "X. Bresson",
            "T. Laurent",
            "Adam Perold",
            "Yann LeCun",
            "Bryan Hooi"
        ],
        "abstract": "Representation learning on text-attributed graphs (TAGs) has become a critical research problem in recent years. A typical example of a TAG is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network (GNN) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models (LMs), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of LLMs with the structural learning capabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs. Our experiments demonstrate that our method achieves state-of-the-art results on well-established TAG datasets, including Cora, PubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond TAGs and holds the potential to enhance other tasks involving graph-text data. Our codes and datasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "tldr": "This work focuses on leveraging LLMs to capture textual information as features, which can be used to boost GNN performance on downstream tasks, and prompts an LLM to perform zero-shot classification, request textual explanations for its decision-making process, and design an LLM-to-LM interpreter to translate these explanations into informative features for downstream GNNs.",
        "venue": "",
        "referenceCount": 51,
        "citationCount": 11,
        "influentialCitationCount": 2
    },
    {
        "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
        "authors": [
            "Jiabin Tang",
            "Yuhao Yang",
            "Wei Wei",
            "Lei Shi",
            "Lixin Su",
            "Suqi Cheng",
            "Dawei Yin",
            "Chao Huang"
        ],
        "abstract": "Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.",
        "tldr": "The GraphGPT framework is presented, which combines a text-graph grounding component to establish a connection between textual information and graph structures, and a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector, that explores self-supervised graph structural signals and task-specific graph instructions.",
        "venue": "arXiv.org",
        "referenceCount": 58,
        "citationCount": 20,
        "influentialCitationCount": 1
    },
    {
        "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
        "authors": [
            "Xiangguo Sun",
            "Hongtao Cheng",
            "Jia Li",
            "Bo Liu",
            "J. Guan"
        ],
        "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
        "tldr": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks.",
        "venue": "Knowledge Discovery and Data Mining",
        "referenceCount": 40,
        "citationCount": 18,
        "influentialCitationCount": 3
    },
    {
        "title": "Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer",
        "authors": [
            "Wen Zhang",
            "Yushan Zhu",
            "Mingyang Chen",
            "Yuxia Geng",
            "Yufen Huang",
            "Yajing Xu",
            "Wenting Song",
            "Hua-zeng Chen"
        ],
        "abstract": "Knowledge graphs (KG) are essential background knowledge providers in many tasks. When designing models for KG-related tasks, one of the key tasks is to devise the Knowledge Representation and Fusion (KRF) module that learns the representation of elements from KGs and fuses them with task representations. While due to the difference of KGs and perspectives to be considered during fusion across tasks, duplicate and ad hoc KRF modules design are conducted among tasks. In this paper, we propose a novel knowledge graph pretraining model KGTransformer that could serve as a uniform KRF module in diverse KG-related tasks. We pretrain KGTransformer with three self-supervised tasks with sampled sub-graphs as input. For utilization, we propose a general prompt-tuning mechanism regarding task data as a triple prompt to allow flexible interactions between task KGs and task data. We evaluate pretrained KGTransformer on three tasks, triple classification, zero-shot image classification, and question answering. KGTransformer consistently achieves better results than specifically designed task models. Through experiments, we justify that the pretrained KGTransformer could be used off the shelf as a general and effective KRF module across KG-related tasks. The code and datasets are available at https://github.com/zjukg/KGTransformer.",
        "tldr": "A novel knowledge graph pretraining model KGTransformer is proposed that could serve as a uniform KRF module in diverse KG-related tasks and consistently achieves better results than specifically designed task models.",
        "venue": "The Web Conference",
        "referenceCount": 71,
        "citationCount": 8,
        "influentialCitationCount": 1
    },
    {
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks",
        "authors": [
            "Zemin Liu",
            "Xingtong Yu",
            "Yuan Fang",
            "Xinming Zhang"
        ],
        "abstract": "Graphs can model complex relationships between objects, enabling a myriad of Web applications such as online page/article classification and social recommendation. While graph neural networks (GNNs) have emerged as a powerful tool for graph representation learning, in an end-to-end supervised setting, their performance heavily relies on a large amount of task-specific supervision. To reduce labeling requirement, the \u201cpre-train, fine-tune\u201d and \u201cpre-train, prompt\u201d paradigms have become increasingly common. In particular, prompting is a popular alternative to fine-tuning in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives in a task-specific manner. However, existing study of prompting on graphs is still limited, lacking a universal treatment to appeal to different downstream tasks. In this paper, we propose GraphPrompt, a novel pre-training and prompting framework on graphs. GraphPrompt not only unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner. Finally, we conduct extensive experiments on five public datasets to evaluate and analyze GraphPrompt.",
        "tldr": "GraphPrompt is proposed, a novel pre- training and prompting framework on graphs that unifies pre-training and downstream tasks into a common task template, but also employs a learnable prompt to assist a downstream task in locating the most relevant knowledge from the pre-trained model in a task-specific manner.",
        "venue": "The Web Conference",
        "referenceCount": 59,
        "citationCount": 27,
        "influentialCitationCount": 7
    },
    {
        "title": "Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction",
        "authors": [
            "Yunzhi Yao",
            "Shengyu Mao",
            "Ningyu Zhang",
            "Xiangnan Chen",
            "Shumin Deng",
            "Xi Chen",
            "Huajun Chen"
        ],
        "abstract": "With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supervised data as a prompt for each sample, which is model-agnostic and can be plugged into widespread existing approaches. Experimental results demonstrate that previous methods integrated with RAP can achieve impressive performance gains in low-resource settings on five datasets of relational triple extraction and event extraction for knowledge graph construction Code is available in https://github.com/zjunlp/RAP.",
        "tldr": "A retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction that can dynamically leverage schema and knowledge inherited from human-annotated and weak-supervised data as a prompt for each sample, which is model-agnostic and can be plugged into widespread existing approaches.",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "referenceCount": 60,
        "citationCount": 2,
        "influentialCitationCount": 0
    },
    {
        "title": "PRODIGY: Enabling In-context Learning Over Graphs",
        "authors": [
            "Qian Huang",
            "Hongyu Ren",
            "Peng Chen",
            "Gregor Krvzmanc",
            "D. Zeng",
            "Percy Liang",
            "J. Leskovec"
        ],
        "abstract": "In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr},etraining \\textbf{O},ver \\textbf{D},iverse \\textbf{I},n-Context \\textbf{G},raph S\\textbf{y},stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph}, representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning.",
        "tldr": "This paper develops PRODIGY, the first pretraining framework that enables in- context learning over graphs with a novel \\emph{prompt graph}, representation, which connects prompt examples and queries and proposes a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 25,
        "citationCount": 13,
        "influentialCitationCount": 5
    },
    {
        "title": "A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges",
        "authors": [
            "Xuansheng Wu",
            "Kaixiong Zhou",
            "Mingchen Sun",
            "Xin Wang",
            "Ninghao Liu"
        ],
        "abstract": "The recent\"pre-train, prompt, predict training\"paradigm has gained popularity as a way to learn generalizable models with limited labeled data. The approach involves using a pre-trained model and a prompting function that applies a template to input samples, adding indicative context and reformulating target tasks as the pre-training task. However, the design of prompts could be a challenging and time-consuming process in complex tasks. The limitation can be addressed by using graph data, as graphs serve as structured knowledge repositories by explicitly modeling the interaction between entities. In this survey, we review prompting methods from the graph perspective, where prompting functions are augmented with graph knowledge. In particular, we introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges. This survey will bridge the gap between graphs and prompt design to facilitate future methodology development.",
        "tldr": "This survey will bridge the gap between graphs and prompt design to facilitate future methodology development and introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges.",
        "venue": "arXiv.org",
        "referenceCount": 51,
        "citationCount": 6,
        "influentialCitationCount": 0
    },
    {
        "title": "Universal Prompt Tuning for Graph Neural Networks",
        "authors": [
            "Taoran Fang",
            "Yunchao Zhang",
            "Yang Yang",
            "Chunping Wang",
            "Lei Chen"
        ],
        "abstract": "In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to obtain the prompted graph for the downstream task in an adaptive manner. We provide rigorous derivations to demonstrate the universality of GPF and make guarantee of its effectiveness. The experimental results under various pre-training strategies indicate that our method performs better than fine-tuning, with an average improvement of about 1.4% in full-shot scenarios and about 3.2% in few-shot scenarios. Moreover, our method significantly outperforms existing specialized prompt-based tuning methods when applied to models utilizing the pre-training strategy they specialize in. These numerous advantages position our method as a compelling alternative to fine-tuning for downstream adaptations.",
        "tldr": "This paper introduces a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy that operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 69,
        "citationCount": 4,
        "influentialCitationCount": 1
    },
    {
        "title": "SGL-PT: A Strong Graph Learner with Graph Prompt Tuning",
        "authors": [
            "Yun Zhu",
            "Jianhao Guo",
            "Siliang Tang"
        ],
        "abstract": "Recently, much exertion has been paid to design graph self-supervised methods to obtain generalized pre-trained models, and adapt pre-trained models onto downstream tasks through fine-tuning. However, there exists an inherent gap between pretext and downstream graph tasks, which insufficiently exerts the ability of pre-trained models and even leads to negative transfer. Meanwhile, prompt tuning has seen emerging success in natural language processing by aligning pre-training and fine-tuning with consistent training objectives. In this paper, we identify the challenges for graph prompt tuning: The first is the lack of a strong and universal pre-training task across sundry pre-training methods in graph domain. The second challenge lies in the difficulty of designing a consistent training objective for both pre-training and downstream tasks. To overcome above obstacles, we propose a novel framework named SGL-PT which follows the learning strategy ``Pre-train, Prompt, and Predict''. Specifically, we raise a strong and universal pre-training task coined as SGL that acquires the complementary merits of generative and contrastive self-supervised graph learning. And aiming for graph classification task, we unify pre-training and fine-tuning by designing a novel verbalizer-free prompting function, which reformulates the downstream task in a similar format as pretext task. Empirical results show that our method surpasses other baselines under unsupervised setting, and our prompt tuning method can greatly facilitate models on biological datasets over fine-tuning methods.",
        "tldr": "A novel framework named SGL-PT is proposed which raises a strong and universal pre- training task coined as SGL that acquires the complementary merits of generative and contrastive self-supervised graph learning and unifies pre-training and fine-tuning.",
        "venue": "arXiv.org",
        "referenceCount": 56,
        "citationCount": 13,
        "influentialCitationCount": 3
    },
    {
        "title": "CodeKGC: Code Language Model for Generative Knowledge Graph Construction",
        "authors": [
            "Zhen Bi",
            "Jing Chen",
            "Yinuo Jiang",
            "Feiyu Xiong",
            "Wei Guo",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "abstract": "Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.",
        "tldr": "This work develops schema-aware prompts that effectively utilize the semantic structure within the knowledge graph that serves as a useful model for prior semantic structural knowledge within the knowledge graph.",
        "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing",
        "referenceCount": 39,
        "citationCount": 11,
        "influentialCitationCount": 1
    },
    {
        "title": "StructGPT: A General Framework for Large Language Model to Reason over Structured Data",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Zican Dong",
            "Keming Ye",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "abstract": "In this paper, we study how to improve the zero-shot reasoning ability of large language models~(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \\emph{Iterative Reading-then-Reasoning~(IRR)}, approach for solving question answering tasks based on structured data, called \\textbf{StructGPT},. In our approach, we construct the specialized function to collect relevant evidence from structured data (\\ie \\emph{reading},), and let LLMs concentrate the reasoning task based on the collected information (\\ie \\emph{reasoning},). Specially, we propose an \\emph{invoking-linearization-generation}, procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/StructGPT},.",
        "tldr": "An Iterative Reading-then-Reasoning approach for solving question answering tasks based on structured data, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 58,
        "citationCount": 60,
        "influentialCitationCount": 8
    },
    {
        "title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
        "authors": [
            "Jiuzhou Han",
            "Nigel Collier",
            "Wray L. Buntine",
            "Ehsan Shareghi"
        ],
        "abstract": "Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.",
        "tldr": "A framework, Prompting with Iterative Verification (PiVe), is proposed, to improve graph-based generative capability of LLMs and shows how a small language model could be trained to act as a verifier module for the output of an LLM, and to iteratively improve its performance via fine-grained corrective instructions.",
        "venue": "arXiv.org",
        "referenceCount": 43,
        "citationCount": 9,
        "influentialCitationCount": 1
    },
    {
        "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
        "authors": [
            "Chengwen Qi",
            "Bowen Li",
            "Binyuan Hui",
            "Bailin Wang",
            "Jinyang Li",
            "Jinwang Wu",
            "Yuanjun Laili"
        ],
        "abstract": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.",
        "tldr": "A new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets is introduced, which suggests that LLMs often resort to shortcut learning and still face challenges on this proposed benchmark.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 43,
        "citationCount": 2,
        "influentialCitationCount": 0
    },
    {
        "title": "Integrating Graphs With Large Language Models: Methods and Prospects",
        "authors": [
            "Shirui Pan",
            "Yizhen Zheng",
            "Yixin Liu",
            "San Murugesan"
        ],
        "abstract": "Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.",
        "tldr": "This article discusses and proposes open questions for integrating LLMs with graph-structured data for the future direction of the field and underscores the pivotal role of graphs in advancing LLMs.",
        "venue": "IEEE Intelligent Systems",
        "referenceCount": 13,
        "citationCount": 5,
        "influentialCitationCount": 0
    },
    {
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "authors": [
            "Jiayan Guo",
            "Lun Du",
            "Hengyu Liu"
        ],
        "abstract": "Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction.",
        "tldr": "Through this study, the current limitations of language models in comprehending graph structures and performing associated reasoning tasks are uncovered and the necessity for further advancements and novel approaches to enhance their graph processing capabilities is emphasized.",
        "venue": "arXiv.org",
        "referenceCount": 44,
        "citationCount": 42,
        "influentialCitationCount": 9
    },
    {
        "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
        "authors": [
            "Wenxuan Ding",
            "Shangbin Feng",
            "Yuhan Liu",
            "Zhaoxuan Tan",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We propose Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. Knowledge Crosswords contains 2,101 individual problems, covering various knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the Knowledge Crosswords benchmark. We additionally propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with hard ones, our proposed Verify-All outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.",
        "tldr": "This work proposes Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints.",
        "venue": "arXiv.org",
        "referenceCount": 63,
        "citationCount": 0,
        "influentialCitationCount": 0
    },
    {
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "authors": [
            "Shirui Pan",
            "Linhao Luo",
            "Yufei Wang",
            "Chen Chen",
            "Jiapu Wang",
            "Xindong Wu"
        ],
        "abstract": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
        "tldr": "A forward-looking roadmap to unify LLMs and KGs together and simultaneously leverage their advantages is presented, which consists of three general frameworks, namely, KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs or for the purpose of enhancing understanding of the knowledge learned by LLMs.",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "referenceCount": 293,
        "citationCount": 137,
        "influentialCitationCount": 6
    },
    {
        "title": "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing",
        "authors": [
            "Shengchao Liu",
            "Weili Nie",
            "Chengpeng Wang",
            "Jiarui Lu",
            "Zhuoran Qiao",
            "Ling Liu",
            "Jian Tang",
            "Chaowei Xiao",
            "Anima Anandkumar"
        ],
        "abstract": "There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language. In experiments, MoleculeSTM obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",
        "tldr": "A multi-modal molecule structure-text model, MoleculeSTM, is presented by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy and obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",
        "venue": "Nat. Mac. Intell.",
        "referenceCount": 84,
        "citationCount": 37,
        "influentialCitationCount": 6
    },
    {
        "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
        "authors": [
            "Jianan Zhao",
            "Meng Qu",
            "Chaozhuo Li",
            "Hao Yan",
            "Qian Liu",
            "Rui Li",
            "Xing Xie",
            "Jian Tang"
        ],
        "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",
        "tldr": "This paper proposes an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 37,
        "citationCount": 45,
        "influentialCitationCount": 12
    },
    {
        "title": "Active Reasoning in an Open-World Environment",
        "authors": [
            "Manjie Xu",
            "Guangyuan Jiang",
            "Weihan Liang",
            "Chi Zhang",
            "Yixin Zhu"
        ],
        "abstract": "Recent advances in vision-language learning have achieved notable success on complete-information question-answering datasets through the integration of extensive world knowledge. Yet, most models operate passively, responding to questions based on pre-stored knowledge. In stark contrast, humans possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions. In response to this gap, we introduce $Conan$, an interactive open-world environment devised for the assessment of active reasoning. $Conan$ facilitates active exploration and promotes multi-round abductive inference, reminiscent of rich, open-world settings like Minecraft. Diverging from previous works that lean primarily on single-round deduction via instruction following, $Conan$ compels agents to actively interact with their surroundings, amalgamating new evidence with prior knowledge to elucidate events from incomplete observations. Our analysis on $Conan$ underscores the shortcomings of contemporary state-of-the-art models in active exploration and understanding complex scenarios. Additionally, we explore Abduction from Deduction, where agents harness Bayesian rules to recast the challenge of abduction as a deductive process. Through $Conan$, we aim to galvanize advancements in active reasoning and set the stage for the next generation of artificial intelligence agents adept at dynamically engaging in environments.",
        "tldr": "This work introduces an interactive open-world environment devised for the assessment of active reasoning, and explores Abduction from Deduction, where agents harness Bayesian rules to recast the challenge of abduction as a deductive process.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 61,
        "citationCount": 1,
        "influentialCitationCount": 0
    },
    {
        "title": "Large Language Models are Visual Reasoning Coordinators",
        "authors": [
            "Liangyu Chen",
            "Boyi Li",
            "Sheng Shen",
            "Jingkang Yang",
            "Chunyuan Li",
            "Kurt Keutzer",
            "Trevor Darrell",
            "Ziwei Liu"
        ],
        "abstract": "Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.",
        "tldr": "This work proposes Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning by facilitating natural language communication that leverages their distinct and complementary capabilities, and validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VL Ms and coordinates them to enable impressive visual reasoning capabilities.",
        "venue": "arXiv.org",
        "referenceCount": 125,
        "citationCount": 13,
        "influentialCitationCount": 0
    },
    {
        "title": "Interactive Visual Reasoning under Uncertainty",
        "authors": [
            "Manjie Xu",
            "Guangyuan Jiang",
            "Chi Zhang",
            "Song-Chun Zhu",
            "Yixin Zhu"
        ],
        "abstract": "One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as\"ivory\") environment for evaluating artificial agents' reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in IVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. The game ends when all uncertainties are resolved or the maximum number of trials is consumed. By evaluating modern artificial agents in IVRE, we notice a clear failure of today's learning methods compared to humans. Such inefficacy in interactive reasoning ability under uncertainty calls for future research in building human-like intelligence.",
        "tldr": "By evaluating modern artificial agents in IVRE, this work notices a clear failure of today's learning methods compared to humans, and inefficacy in interactive reasoning ability under uncertainty calls for future research in building human-like intelligence.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 84,
        "citationCount": 2,
        "influentialCitationCount": 0
    },
    {
        "title": "Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning",
        "authors": [
            "Xiaoqian Wu",
            "Yong-Lu Li",
            "Jianhua Sun",
            "Cewu Lu"
        ],
        "abstract": "Human reasoning can be understood as a cooperation between the intuitive, associative\"System-1\"and the deliberative, logical\"System-2\". For existing System-1-like methods in visual activity understanding, it is crucial to integrate System-2 processing to improve explainability, generalization, and data efficiency. One possible path of activity reasoning is building a symbolic system composed of symbols and rules, where one rule connects multiple symbols, implying human knowledge and reasoning abilities. Previous methods have made progress, but are defective with limited symbols from handcraft and limited rules from visual-based annotations, failing to cover the complex patterns of activities and lacking compositional generalization. To overcome the defects, we propose a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules. Collecting massive human knowledge via manual annotations is expensive to instantiate this symbolic system. Instead, we leverage the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties, i.e., Symbols from Large Language Models (Symbol-LLM). Then, given an image, visual contents from the images are extracted and checked as symbols and activity semantics are reasoned out based on rules via fuzzy logic calculation. Our method shows superiority in extensive activity understanding tasks. Code and data are available at https://mvig-rhos.com/symbol_llm.",
        "tldr": "This work proposes a new symbolic system with two ideal important properties: broad-coverage symbols and rational rules, and leverages the recent advancement of LLMs (Large Language Models) as an approximation of the two ideal properties.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 48,
        "citationCount": 0,
        "influentialCitationCount": 0
    },
    {
        "title": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use",
        "authors": [
            "Yonatan Bitton",
            "Hritik Bansal",
            "Jack Hessel",
            "Rulin Shao",
            "Wanrong Zhu",
            "Anas Awadalla",
            "Josh Gardner",
            "Rohan Taori",
            "L. Schimdt"
        ],
        "abstract": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io.",
        "tldr": "This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.",
        "venue": "arXiv.org",
        "referenceCount": 99,
        "citationCount": 22,
        "influentialCitationCount": 1
    },
    {
        "title": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models",
        "authors": [
            "Neel Guha",
            "Julian Nyarko",
            "Daniel E. Ho",
            "Christopher R\u00e9",
            "Adam Chilton",
            "Aditya Narayana",
            "Alex Chohlas-Wood",
            "Austin M. K. Peters",
            "Brandon Waldon",
            "D. Rockmore",
            "Diego A. Zambrano",
            "Dmitry Talisman",
            "E. Hoque",
            "Faiz Surani",
            "F. Fagan",
            "Galit Sarfaty",
            "Gregory M. Dickinson",
            "Haggai Porat",
            "Jason Hegland",
            "Jessica Wu",
            "Joe Nudell",
            "Joel Niklaus",
            "John J. Nay",
            "Jonathan H. Choi",
            "K. Tobia",
            "M. Hagan",
            "Megan Ma",
            "Michael A. Livermore",
            "Nikon Rasumov-Rahe",
            "Nils Holzenberger",
            "Noam Kolt",
            "Peter Henderson",
            "Sean Rehaag",
            "Sharad Goel",
            "Shangsheng Gao",
            "Spencer Williams",
            "Sunny G. Gandhi",
            "Tomer Zur",
            "Varun J. Iyer",
            "Zehua Li"
        ],
        "abstract": "The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.",
        "tldr": "To enable cross-disciplinary conversations about LLMs in the law, it is shown how popular legal frameworks for describing legal reasoning correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary.",
        "venue": "Social Science Research Network",
        "referenceCount": 59,
        "citationCount": 20,
        "influentialCitationCount": 3
    },
    {
        "title": "Training Verifiers to Solve Math Word Problems",
        "authors": [
            "Karl Cobbe",
            "V. Kosaraju",
            "Mohammad Bavarian",
            "Mark Chen",
            "Heewoo Jun",
            "Lukasz Kaiser",
            "Matthias Plappert",
            "Jerry Tworek",
            "Jacob Hilton",
            "Reiichiro Nakano",
            "Christopher Hesse",
            "John Schulman"
        ],
        "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
        "tldr": "It is demonstrated that verification significantly improves performance on GSM8K, and there is strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
        "venue": "arXiv.org",
        "referenceCount": 31,
        "citationCount": 1030,
        "influentialCitationCount": 286
    },
    {
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
        "authors": [
            "Pan Lu",
            "Hritik Bansal",
            "Tony Xia",
            "Jiacheng Liu",
            "Chun-yue Li",
            "Hannaneh Hajishirzi",
            "Hao Cheng",
            "Kai-Wei Chang",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",
        "tldr": "The in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning, but it still falls short of human performance by 10.4%, which underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks.",
        "venue": "",
        "referenceCount": 89,
        "citationCount": 36,
        "influentialCitationCount": 3
    },
    {
        "title": "Teaching Algorithmic Reasoning via In-context Learning",
        "authors": [
            "Hattie Zhou",
            "Azade Nova",
            "H. Larochelle",
            "Aaron C. Courville",
            "Behnam Neyshabur",
            "Hanie Sedghi"
        ],
        "abstract": "Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.",
        "tldr": "This work shows that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which it is referred to as algorithmic prompting, and evaluates the approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrates significant boosts in performance.",
        "venue": "arXiv.org",
        "referenceCount": 68,
        "citationCount": 68,
        "influentialCitationCount": 10
    },
    {
        "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
        "authors": [
            "Yufei Tian",
            "Abhilasha Ravichander",
            "Lianhui Qin",
            "R. L. Bras",
            "Raja Marjieh",
            "Nanyun Peng",
            "Yejin Choi",
            "Thomas L. Griffiths",
            "Faeze Brahman"
        ],
        "abstract": "We explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting. The setting requires circumventing a cognitive bias known in psychology as ''functional fixedness'' to use familiar objects in innovative or unconventional ways. To this end, we create MacGyver, an automatically generated dataset consisting of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking 'out-of-the-box'. We then present our collection of problems to both LLMs and humans to compare and contrast their problem-solving abilities. We show that MacGyver is challenging for both groups, but in unique and complementary ways. For example, humans typically excel in solving problems that they are familiar with but may struggle with tasks requiring domain-specific knowledge, leading to a higher variance. On the other hand, LLMs, being exposed to a variety of highly specialized knowledge, attempt broader problems but are prone to overconfidence and propose actions that are physically infeasible or inefficient. We also provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work provides insight into the creative problem-solving capabilities of humans and AI and illustrates how psychological paradigms can be extended into large-scale tasks for comparing humans and machines.",
        "tldr": "Insight is provided into the creative problem-solving capabilities of humans and AI and how psychological paradigms can be extended into large-scale tasks for comparing humans and machines is illustrated.",
        "venue": "arXiv.org",
        "referenceCount": 35,
        "citationCount": 0,
        "influentialCitationCount": 0
    },
    {
        "Cannot_Find": "ARXIV:2310.16427"
    },
    {
        "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
        "authors": [
            "John X. Morris",
            "Chandan Singh",
            "Alexander M. Rush",
            "Jianfeng Gao",
            "Yuntian Deng"
        ],
        "abstract": "Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based finetuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model's decision-making process.",
        "tldr": "Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning, and variants of Tree Prompting allow inspection of a model's decision-making process.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 60,
        "citationCount": 4,
        "influentialCitationCount": 1
    },
    {
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "authors": [
            "Jason Wei",
            "Xuezhi Wang",
            "Dale Schuurmans",
            "Maarten Bosma",
            "E. Chi",
            "F. Xia",
            "Quoc Le",
            "Denny Zhou"
        ],
        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
        "tldr": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 118,
        "citationCount": 2958,
        "influentialCitationCount": 452
    },
    {
        "title": "Why think step-by-step? Reasoning emerges from the locality of experience",
        "authors": [
            "Ben Prystawski",
            "Michael Y. Li",
            "Noah D. Goodman"
        ],
        "abstract": "Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite the fact that we get no additional data from the world. Similarly, when large language models generate a series of intermediate steps (a chain of thought) before answering a question, they often produce better answers than they otherwise would. We investigate why and how chain-of-thought reasoning is useful in language models, testing the hypothesis that reasoning is effective when training data consists of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We prove that there will exist a\"reasoning gap\", where reasoning through intermediate variables improves inference, for the simple case of an autoregressive density estimator trained on local samples from a chain-structured probabilistic model. We then test our hypothesis empirically in more complex models, training an autoregressive language model on samples from Bayes nets but only including a subset of variables in each sample. We test language models' ability to match conditional probabilities with and without intermediate reasoning steps, finding that intermediate steps are only helpful when the training data is locally structured with respect to dependencies between variables and that the combination of locally-structured observations and reasoning is much more data-efficient than training on all variables. Our results illustrate how the effectiveness of reasoning step by step is rooted in the local statistical structure of the training data.",
        "tldr": "This work proves that there will exist a \"reasoning gap\", where reasoning through intermediate variables improves inference, for an autoregressive density estimator trained on local samples from a chain-structured probabilistic model, and test language models' ability to match conditional probabilities with and without intermediate reasoning steps.",
        "venue": "arXiv.org",
        "referenceCount": 27,
        "citationCount": 21,
        "influentialCitationCount": 2
    },
    {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "T. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
        "tldr": "A new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 52,
        "citationCount": 460,
        "influentialCitationCount": 58
    },
    {
        "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
        "authors": [
            "Maciej Besta",
            "Nils Blach",
            "Ale\u0161 Kub\u00ed\u010dek",
            "Robert Gerstenberger",
            "Lukas Gianinazzi",
            "Joanna Gajda",
            "Tomasz Lehmann",
            "Michal Podstawski",
            "H. Niewiadomski",
            "P. Nyczyk",
            "Torsten Hoefler"
        ],
        "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by>31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.",
        "tldr": "Graph of Thoughts is introduced: a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts, and is ensured that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes.",
        "venue": "arXiv.org",
        "referenceCount": 85,
        "citationCount": 124,
        "influentialCitationCount": 4
    },
    {
        "Cannot_Find": "ARXIV:2310.04562"
    },
    {
        "title": "Large Language Models can Learn Rules",
        "authors": [
            "Zhaocheng Zhu",
            "Yuan Xue",
            "Xinyun Chen",
            "Denny Zhou",
            "Jian Tang",
            "D. Schuurmans",
            "Hanjun Dai"
        ],
        "abstract": "When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.",
        "tldr": "Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with large language models, improves existing prompting methods, with an absolute gain of 11-27% in accuracy.",
        "venue": "arXiv.org",
        "referenceCount": 74,
        "citationCount": 10,
        "influentialCitationCount": 0
    },
    {
        "title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples",
        "authors": [
            "Abulhair Saparov",
            "Richard Yuanzhe Pang",
            "Vishakh Padmakumar",
            "Nitish Joshi",
            "Seyed Mehran Kazemi",
            "Najoung Kim",
            "He He"
        ],
        "abstract": "Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.",
        "tldr": "A new synthetic and programmable reasoning dataset is constructed that enables control over deduction rules and proof complexity and shows that large language models have difficulty generalizing to longer proofs, but are able to generalize to compositional proofs.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 41,
        "citationCount": 19,
        "influentialCitationCount": 0
    },
    {
        "title": "SPRING: Studying the Paper and Reasoning to Play Games",
        "authors": [
            "Yue Wu",
            "So Yeon Min",
            "Shrimai Prabhumoye",
            "Yonatan Bisk",
            "R. Salakhutdinov",
            "A. Azaria",
            "Tom M. Mitchell",
            "Yuan-Fang Li"
        ],
        "abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\"reasoning\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.",
        "tldr": "The experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories, and show the potential of games as a test bed for LLMs.",
        "venue": "",
        "referenceCount": 0,
        "citationCount": 13,
        "influentialCitationCount": 1
    },
    {
        "title": "Parsel\ud83e\udd86: Algorithmic Reasoning with Language Models by Composing Decompositions",
        "authors": [
            "E. Zelikman",
            "Qian Huang",
            "Gabriel Poesia",
            "Noah D. Goodman",
            "N. Haber"
        ],
        "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel",
        "tldr": "This work introduces Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, which automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 76,
        "citationCount": 6,
        "influentialCitationCount": 1
    },
    {
        "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
        "authors": [
            "Ge Zheng",
            "Bin Yang",
            "Jiajin Tang",
            "Hong-Yu Zhou",
            "Sibei Yang"
        ],
        "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
        "tldr": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 75,
        "citationCount": 9,
        "influentialCitationCount": 1
    },
    {
        "title": "Deductive Verification of Chain-of-Thought Reasoning",
        "authors": [
            "Z. Ling",
            "Yunhao Fang",
            "Xuanlin Li",
            "Zhiao Huang",
            "Mingu Lee",
            "R. Memisevic",
            "Hao Su"
        ],
        "abstract": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify_cot.",
        "tldr": "This work proposes Natural Program, a natural language-based deductive reasoning format that enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps and significantly enhances the rigor and trustfulness of generated reasoning steps.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 71,
        "citationCount": 38,
        "influentialCitationCount": 3
    },
    {
        "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
        "authors": [
            "Karthik Valmeekam",
            "Alberto Olmo",
            "S. Sreedharan",
            "Subbarao Kambhampati"
        ],
        "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
        "tldr": "This work proposes PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 35,
        "citationCount": 75,
        "influentialCitationCount": 10
    },
    {
        "title": "Self-Evaluation Guided Beam Search for Reasoning",
        "authors": [
            "Yuxi Xie",
            "Kenji Kawaguchi",
            "Yiran Zhao",
            "Xu Zhao",
            "MingSung Kan",
            "Junxian He",
            "Qizhe Xie"
        ],
        "abstract": "Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.",
        "tldr": "A stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs through stochastic beam search and a decoding algorithm integrating the self- evaluation guidance via stochastically beam search are proposed.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 52,
        "citationCount": 19,
        "influentialCitationCount": 3
    },
    {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Y. Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%, lifting the state of the art to 98.78%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.",
        "tldr": "This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 72,
        "citationCount": 128,
        "influentialCitationCount": 17
    },
    {
        "title": "Large Language Models are Zero-Shot Reasoners",
        "authors": [
            "Takeshi Kojima",
            "S. Gu",
            "Machel Reid",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
        "tldr": "Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 60,
        "citationCount": 1527,
        "influentialCitationCount": 169
    },
    {
        "title": "Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models",
        "authors": [
            "Ben Prystawski",
            "P. Thibodeau",
            "Noah D. Goodman"
        ],
        "abstract": "Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.",
        "tldr": "This paper uses chain-of-thought prompts to introduce structures from probabilistic models into large language models, and applies these prompts to the two largest versions of GPT-3 and shows that they can improve performance in a paraphrase selection task.",
        "venue": "arXiv.org",
        "referenceCount": 27,
        "citationCount": 10,
        "influentialCitationCount": 0
    },
    {
        "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
        "authors": [
            "Freda Shi",
            "Mirac Suzgun",
            "Markus Freitag",
            "Xuezhi Wang",
            "Suraj Srivats",
            "Soroush Vosoughi",
            "Hyung Won Chung",
            "Yi Tay",
            "Sebastian Ruder",
            "Denny Zhou",
            "Dipanjan Das",
            "Jason Wei"
        ],
        "abstract": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
        "tldr": "It is found that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 53,
        "citationCount": 126,
        "influentialCitationCount": 20
    },
    {
        "Cannot_Find": "ARXIV:2210.06710"
    },
    {
        "title": "Language Models of Code are Few-Shot Commonsense Learners",
        "authors": [
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Yiming Yang",
            "Graham Neubig"
        ],
        "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches \u2018serialize\u2019 the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",
        "tldr": "This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 38,
        "citationCount": 107,
        "influentialCitationCount": 6
    },
    {
        "title": "PAL: Program-aided Language Models",
        "authors": [
            "Luyu Gao",
            "Aman Madaan",
            "Shuyan Zhou",
            "Uri Alon",
            "Pengfei Liu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
        "tldr": "This paper presents Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.",
        "venue": "International Conference on Machine Learning",
        "referenceCount": 67,
        "citationCount": 234,
        "influentialCitationCount": 46
    },
    {
        "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
        "authors": [
            "Wenhu Chen",
            "Xueguang Ma",
            "Xinyi Wang",
            "William W. Cohen"
        ],
        "abstract": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts",
        "tldr": "Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoTA performance on all math problem datasets and near-SoTAperformance on financial datasets.",
        "venue": "arXiv.org",
        "referenceCount": 49,
        "citationCount": 300,
        "influentialCitationCount": 46
    },
    {
        "Cannot_Find": "ARXIV:2301.00303"
    },
    {
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
        "authors": [
            "Xuezhi Wang",
            "Jason Wei",
            "D. Schuurmans",
            "Quoc Le",
            "E. Chi",
            "Denny Zhou"
        ],
        "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
        "tldr": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 80,
        "citationCount": 1136,
        "influentialCitationCount": 204
    },
    {
        "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
        "authors": [
            "Yifei Li",
            "Zeqi Lin",
            "Shizhuo Zhang",
            "Qiang Fu",
            "B. Chen",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
        "tldr": "This paper presents DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models and achieves new state-of-the-art results on six of eight reasoning benchmarks.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "referenceCount": 70,
        "citationCount": 65,
        "influentialCitationCount": 6
    },
    {
        "title": "Automatic Chain of Thought Prompting in Large Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Alexander J. Smola"
        ],
        "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",
        "tldr": "An automatic CoT prompting method that samples questions with diversity and generates reasoning chains to construct demonstrations and consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 39,
        "citationCount": 269,
        "influentialCitationCount": 44
    },
    {
        "title": "Complexity-Based Prompting for Multi-Step Reasoning",
        "authors": [
            "Yao Fu",
            "Hao-Chun Peng",
            "Ashish Sabharwal",
            "Peter Clark",
            "Tushar Khot"
        ],
        "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
        "tldr": "This work proposes complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning that substantially improves multi- step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks and two BigBenchHard tasks.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 41,
        "citationCount": 186,
        "influentialCitationCount": 31
    },
    {
        "title": "Automatic Engineering of Long Prompts",
        "authors": [
            "Cho-Jui Hsieh",
            "Si Si",
            "Felix X. Yu",
            "I. Dhillon"
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts. However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort. Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences. However, the automatic design of long prompts remains a challenging problem due to its immense search space. In this paper, we investigate the performance of greedy algorithms and genetic algorithms for automatic long prompt engineering. We demonstrate that a simple greedy approach with beam search outperforms other methods in terms of search efficiency. Moreover, we introduce two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our search algorithm. Our results show that the proposed automatic long prompt engineering algorithm achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.",
        "tldr": "It is demonstrated that a simple greedy approach with beam search outperforms other methods in terms of search efficiency and two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in the authors' search algorithm are introduced.",
        "venue": "arXiv.org",
        "referenceCount": 32,
        "citationCount": 1,
        "influentialCitationCount": 0
    },
    {
        "title": "Large Language Models are Better Reasoners with Self-Verification",
        "authors": [
            "Yixuan Weng",
            "Minjun Zhu",
            "Fei Xia",
            "Bin Li",
            "Shizhu He",
            "Kang Liu",
            "Jun Zhao"
        ],
        "abstract": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
        "tldr": "This paper proposes and proves that LLMs also have similar self-verification abilities, and takes the conclusion obtained by CoT as one of the conditions for solving the original problem.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 55,
        "citationCount": 43,
        "influentialCitationCount": 4
    },
    {
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "authors": [
            "Denny Zhou",
            "Nathanael Scharli",
            "Le Hou",
            "Jason Wei",
            "Nathan Scales",
            "Xuezhi Wang",
            "D. Schuurmans",
            "O. Bousquet",
            "Quoc Le",
            "E. Chi"
        ],
        "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
        "tldr": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 73,
        "citationCount": 513,
        "influentialCitationCount": 47
    },
    {
        "title": "Compositional Semantic Parsing with Large Language Models",
        "authors": [
            "Andrew Drozdov",
            "Nathanael Scharli",
            "Ekin Akyuurek",
            "Nathan Scales",
            "Xinying Song",
            "Xinyun Chen",
            "O. Bousquet",
            "Denny Zhou"
        ],
        "abstract": "Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",
        "tldr": "The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse.",
        "venue": "arXiv.org",
        "referenceCount": 62,
        "citationCount": 66,
        "influentialCitationCount": 5
    },
    {
        "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
        "authors": [
            "Tushar Khot",
            "H. Trivedi",
            "Matthew Finlayson",
            "Yao Fu",
            "Kyle Richardson",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",
        "tldr": "It is shown that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3 and to incorporate a symbolic information retrieval within the decomposition framework, leading to improved performance on both tasks.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 114,
        "citationCount": 168,
        "influentialCitationCount": 11
    },
    {
        "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
        "authors": [
            "Ofir Press",
            "Muru Zhang",
            "Sewon Min",
            "Ludwig Schmidt",
            "Noah A. Smith",
            "M. Lewis"
        ],
        "abstract": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",
        "tldr": "It is shown that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease, and while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform compositional reasoning.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 65,
        "citationCount": 257,
        "influentialCitationCount": 43
    },
    {
        "title": "Successive Prompting for Decomposing Complex Questions",
        "authors": [
            "Dheeru Dua",
            "Shivanshu Gupta",
            "Sameer Singh",
            "Matt Gardner"
        ],
        "abstract": "Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce \u201cSuccessive Prompting\u201d where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model\u2019s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",
        "tldr": "A way to generate synthetic dataset which can be used to bootstrap model\u2019s ability to decompose and answer intermediate questions is introduced and achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 42,
        "citationCount": 60,
        "influentialCitationCount": 4
    },
    {
        "title": "Large Language Models are Versatile Decomposers: Decomposing Evidence and Questions for Table-based Reasoning",
        "authors": [
            "Yunhu Ye",
            "Binyuan Hui",
            "Min Yang",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "abstract": "Table-based reasoning has shown remarkable progress in a wide range of table-based tasks. It is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ''huge'' evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ''huge'' evidence. Second, we propose a novel ''parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge to produce numerical and logical sub-questions with a powerful LLM. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. Extensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. Notably, our method outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions. For reproducibility, we release our source code and data at: https://github.com/AlibabaResearch/DAMO-ConvAI.",
        "tldr": "This work exploits large language models (LLMs) as decomposers for effective table-based reasoning, and proposes a novel ''parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge.",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "referenceCount": 60,
        "citationCount": 49,
        "influentialCitationCount": 8
    },
    {
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
        "authors": [
            "Antonia Creswell",
            "M. Shanahan",
            "I. Higgins"
        ],
        "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
        "tldr": "A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 52,
        "citationCount": 193,
        "influentialCitationCount": 18
    },
    {
        "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
        "authors": [
            "Linlu Qiu",
            "Liwei Jiang",
            "Ximing Lu",
            "Melanie Sclar",
            "Valentina Pyatkin",
            "Chandra Bhagavatula",
            "Bailin Wang",
            "Yoon Kim",
            "Yejin Choi",
            "Nouha Dziri",
            "Xiang Ren"
        ],
        "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
        "tldr": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting.",
        "venue": "arXiv.org",
        "referenceCount": 88,
        "citationCount": 11,
        "influentialCitationCount": 0
    },
    {
        "title": "Large Language Models can Learn Rules",
        "authors": [
            "Zhaocheng Zhu",
            "Yuan Xue",
            "Xinyun Chen",
            "Denny Zhou",
            "Jian Tang",
            "D. Schuurmans",
            "Hanjun Dai"
        ],
        "abstract": "When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.",
        "tldr": "Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with large language models, improves existing prompting methods, with an absolute gain of 11-27% in accuracy.",
        "venue": "arXiv.org",
        "referenceCount": 73,
        "citationCount": 10,
        "influentialCitationCount": 0
    },
    {
        "title": "ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning",
        "authors": [
            "Linhao Luo",
            "Jiaxin Ju",
            "Bo Xiong",
            "Yuan-Fang Li",
            "Gholamreza Haffari",
            "Shirui Pan"
        ],
        "abstract": "Logical rules are essential for uncovering the logical connections between relations, which could improve reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, the ranked rules can be used to conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.",
        "tldr": "A novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs, initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules.",
        "venue": "arXiv.org",
        "referenceCount": 47,
        "citationCount": 5,
        "influentialCitationCount": 0
    },
    {
        "Cannot_Find": "ARXIV:2310.01074"
    },
    {
        "title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering",
        "authors": [
            "Xiusi Chen",
            "Jyun-Yu Jiang",
            "Wei-Cheng Chang",
            "Cho-Jui Hsieh",
            "Hsiang-Fu Yu",
            "Wei Wang"
        ],
        "abstract": "Few-shot question answering (QA) aims at achieving satisfactory results on machine question answering when only a few training samples are available. Recent advances mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing improvements in F-1 scores by up to 27.5%.",
        "tldr": "MinPrompt is presented, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation that is able to achieve comparable or better results than baselines with a high degree of efficiency.",
        "venue": "arXiv.org",
        "referenceCount": 32,
        "citationCount": 2,
        "influentialCitationCount": 1
    },
    {
        "title": "Unifying Structure Reasoning and Language Pre-training for Complex Reasoning Tasks",
        "authors": [
            "Siyuan Wang",
            "Zhongyu Wei",
            "Jiarong Xu",
            "Zhihao Fan"
        ],
        "abstract": "Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four datasets demonstrate that the proposed model achieves significant improvements in complex reasoning tasks involving diverse structures, and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality.",
        "tldr": "A unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill is proposed and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality.",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "referenceCount": 52,
        "citationCount": 1,
        "influentialCitationCount": 0
    },
    {
        "title": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models",
        "authors": [
            "Sivan Doveh",
            "Assaf Arbelle",
            "Sivan Harary",
            "Roei Herzig",
            "Donghyun Kim",
            "Paola Cascante-Bonilla",
            "Amit Alfassy",
            "R. Panda",
            "R. Giryes",
            "R. Feris",
            "S. Ullman",
            "Leonid Karlinsky"
        ],
        "abstract": "Vision and Language (VL) models offer an effective method for aligning representation spaces of images and text, leading to numerous applications such as cross-modal retrieval, visual question answering, captioning, and more. However, the aligned image-text spaces learned by all the popular VL models are still suffering from the so-called `object bias' - their representations behave as `bags of nouns', mostly ignoring or downsizing the attributes, relations, and states of objects described/appearing in texts/images. Although some great attempts at fixing these `compositional reasoning' issues were proposed in the recent literature, the problem is still far from being solved. In this paper, we uncover two factors limiting the VL models' compositional reasoning performance. These two factors are properties of the paired VL dataset used for finetuning and pre-training the VL model: (i) the caption quality, or in other words `image-alignment', of the texts; and (ii) the `density' of the captions in the sense of mentioning all the details appearing on the image. We propose a fine-tuning approach for automatically treating these factors leveraging a standard VL dataset (CC3M). Applied to CLIP, we demonstrate its significant compositional reasoning performance increase of up to $\\sim27\\%$ over the base model, up to $\\sim20\\%$ over the strongest baseline, and by $6.7\\%$ on average.",
        "tldr": "A fine-tuning approach is proposed for automatically treating two factors limiting the VL models' compositional reasoning performance: the caption quality, or in other words `image-alignment', of the texts and the `density' of the captions in the sense of mentioning all the details appearing on the image.",
        "venue": "arXiv.org",
        "referenceCount": 86,
        "citationCount": 8,
        "influentialCitationCount": 2
    },
    {
        "title": "SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning",
        "authors": [
            "Yunxiang Zhang",
            "Xiaojun Wan"
        ],
        "abstract": "Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our dataset is publicly available at https://github.com/yunx-z/situated_gen.",
        "tldr": "This work introduces a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor and formalizes this challenging task as SituatedGen.",
        "venue": "arXiv.org",
        "referenceCount": 61,
        "citationCount": 1,
        "influentialCitationCount": 0
    },
    {
        "title": "RECKONING: Reasoning through Dynamic Knowledge Encoding",
        "authors": [
            "Zeming Chen",
            "Gail Weiss",
            "E. Mitchell",
            "Asli Celikyilmaz",
            "Antoine Bosselut"
        ],
        "abstract": "Recent studies on transformer-based language models show that they can answer questions by reasoning over knowledge provided as part of the context (i.e., in-context reasoning). However, since the available knowledge is often not filtered for a particular question, in-context reasoning can be sensitive to distractor facts, additional content that is irrelevant to a question but that may be relevant for a different question (i.e., not necessarily random noise). In these situations, the model fails to distinguish the knowledge that is necessary to answer the question, leading to spurious reasoning and degraded performance. This reasoning failure contrasts with the model's apparent ability to distinguish its contextual knowledge from all the knowledge it has memorized during pre-training. Following this observation, we propose teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question. Our method, RECKONING, is a bi-level learning algorithm that teaches language models to reason by updating their parametric knowledge through back-propagation, allowing them to then answer questions using the updated parameters. During training, the inner loop rapidly adapts a copy of the model weights to encode contextual knowledge into its parameters. In the outer loop, the model learns to use the updated weights to reproduce and answer reasoning questions about the memorized knowledge. Our experiments on two multi-hop reasoning datasets show that RECKONING's performance improves over the in-context reasoning baseline (by up to 4.5%). We also find that compared to in-context reasoning, RECKONING generalizes better to longer reasoning chains unseen during training, is more robust to distractors in the context, and is more computationally efficient when multiple questions are asked about the same knowledge.",
        "tldr": "This paper proposes teaching the model to reason more robustly by folding the provided contextual knowledge into the model's parameters before presenting it with a question, and shows that RECKONING's performance improves over the in-context reasoning baseline and generalizes better to longer reasoning chains unseen during training.",
        "venue": "arXiv.org",
        "referenceCount": 86,
        "citationCount": 5,
        "influentialCitationCount": 0
    },
    {
        "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
        "authors": [
            "Minki Kang",
            "Seanie Lee",
            "Jinheon Baek",
            "Kenji Kawaguchi",
            "Sung Ju Hwang"
        ],
        "abstract": "Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small Language Models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantly improves the performance of small T5 and GPT models on the challenging knowledge-intensive reasoning datasets, namely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the 250M T5 models achieve superior performance against the fine-tuned 3B models, having 12 times larger parameters, on both MedQA-USMLE and StrategyQA benchmarks.",
        "tldr": "This work proposes Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales obtained from LLMs with augmented knowledge retrieved from an external knowledge base, and proposes a neural reranker to obtain documents relevant to rationale generation.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 59,
        "citationCount": 7,
        "influentialCitationCount": 1
    },
    {
        "title": "Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning",
        "authors": [
            "Xiaoming Shi",
            "Siqiao Xue",
            "Kangrui Wang",
            "Fan Zhou",
            "James Y. Zhang",
            "Jun-ping Zhou",
            "Chenhao Tan",
            "Hongyuan Mei"
        ],
        "abstract": "Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction performance of event sequence models. We design LAMP, a framework that integrates a large language model in event prediction. Particularly, the language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on several challenging real-world datasets, we demonstrate that our framework -- thanks to the reasoning capabilities of large language models -- could significantly outperform the state-of-the-art event sequence models.",
        "tldr": "This paper designs LAMP, a framework that integrates a large language model in event prediction, and demonstrates that this framework could significantly outperform the state-of-the-art event sequence models.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 86,
        "citationCount": 11,
        "influentialCitationCount": 1
    },
    {
        "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information",
        "authors": [
            "Mehran Kazemi",
            "Quan Yuan",
            "Deepti Bhatia",
            "Najoung Kim",
            "Xin Xu",
            "Vaiva Imbrasaite",
            "Deepak Ramachandran"
        ],
        "abstract": "Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.",
        "tldr": "A significant gap is revealed in the reasoning capacity of state-of-the-art LMs on the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and a dataset is developed called BoardgameQA for measuring the reasoningcapacity of LMs in this setting.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 60,
        "citationCount": 6,
        "influentialCitationCount": 4
    },
    {
        "title": "CLadder: Assessing Causal Reasoning in Language Models",
        "authors": [
            "Zhijing Jin",
            "Yuen Chen",
            "Felix Leeb",
            "Luigi Gresele",
            "Ojasv Kamal",
            "Zhiheng Lyu",
            "Kevin Blin",
            "Fernando Gonzalez Adauto",
            "Max Kleiman-Weiner",
            "Mrinmaya Sachan",
            "Bernhard Scholkopf"
        ],
        "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
        "tldr": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al.",
        "venue": "",
        "referenceCount": 100,
        "citationCount": 7,
        "influentialCitationCount": 2
    },
    {
        "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
        "authors": [
            "K. Bhatia",
            "A. Narayan",
            "Chris De Sa",
            "Christopher R\u00e9"
        ],
        "abstract": "Large language models (LLMs) exhibit in-context learning abilities which enable the same model to perform several tasks without any task-specific training. In contrast, traditional adaptation approaches, such as fine-tuning, modify the underlying models for each specific task. In-context learning, however, consistently underperforms task-specific tuning approaches even when presented with the same examples. While most existing approaches (e.g., prompt engineering) focus on the LLM's learned representations to patch this performance gap, our analysis actually reveal that LLM representations contain sufficient information to make good predictions. As such, we focus on the LLM's reasoning abilities and demonstrate that this performance gap exists due to their inability to perform simple probabilistic reasoning tasks. This raises an intriguing question: Are LLMs actually capable of learning how to reason in a task-agnostic manner? We answer this in the affirmative and propose TART which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module. TART trains this reasoning module in a task-agnostic manner using only synthetic logistic regression tasks and composes it with an arbitrary real-world pre-trained model without any additional training. With a single inference module, TART improves performance across different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M - 6B), tasks (14 NLP binary classification tasks), and even across different modalities (audio and vision). Additionally, on the RAFT Benchmark, TART improves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B), and is within 4% of GPT-3 (175B). Our code and models are available at https://github.com/HazyResearch/TART .",
        "tldr": "This work proposes TART, which generically improves an LLM's reasoning abilities using a synthetically trained Transformer-based reasoning module in a task-agnostic manner and improves performance across different model families, tasks, and even across different modalities.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 54,
        "citationCount": 2,
        "influentialCitationCount": 0
    },
    {
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "authors": [
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Bo Tian",
            "Peng Wang",
            "Shumin Deng",
            "Meng Wang",
            "Zekun Xi",
            "Shengyu Mao",
            "Jintian Zhang",
            "Yuansheng Ni",
            "Siyuan Cheng",
            "Ziwen Xu",
            "Xin Xu",
            "Jia-Chen Gu",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Lei Liang",
            "Zhiqiang Zhang",
            "Xiao-Jun Zhu",
            "Jun Zhou",
            "Huajun Chen"
        ],
        "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.",
        "tldr": "A unified categorization criterion is proposed that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge and an in-depth analysis of knowledge location is provided, which can give a deeper understanding of the knowledge structures inherent within LLMs.",
        "venue": "arXiv.org",
        "referenceCount": 243,
        "citationCount": 7,
        "influentialCitationCount": 0
    },
    {
        "title": "How FaR Are Large Language Models From Agents with Theory-of-Mind?",
        "authors": [
            "Pei Zhou",
            "Aman Madaan",
            "Srividya Pranavi Potharaju",
            "Aditya Gupta",
            "Kevin R. McKee",
            "Ari Holtzman",
            "J. Pujara",
            "Xiang Ren",
            "Swaroop Mishra",
            "Aida Nematzadeh",
            "Shyam Upadhyay",
            "Manaal Faruqui"
        ],
        "abstract": "\"Thinking is for Doing.\"Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.",
        "tldr": "A zero-shot prompting framework, Foresee and Reflect (FaR), is introduced, which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions, and generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action.",
        "venue": "arXiv.org",
        "referenceCount": 52,
        "citationCount": 13,
        "influentialCitationCount": 1
    },
    {
        "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
        "authors": [
            "Shima Rahimi Moghaddam",
            "C. Honey"
        ],
        "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
        "tldr": "It is demonstrated that appropriate prompting enhances LLM ToM reasoning, and the context-dependent nature of LLM cognitive capacities is underscored, as it is found that LLMs trained with Reinforcement Learning from Human Feedback improved their ToM accuracy via in-context learning.",
        "venue": "arXiv.org",
        "referenceCount": 71,
        "citationCount": 40,
        "influentialCitationCount": 2
    },
    {
        "title": "ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind",
        "authors": [
            "Yuan-Fang Wang",
            "Fangwei Zhong",
            "Jing Xu",
            "Yizhou Wang"
        ],
        "abstract": "Being able to predict the mental states of others is a key factor to effective social interaction. It is also crucial for distributed multi-agent systems, where agents are required to communicate and cooperate. In this paper, we introduce such an important social-cognitive skill, i.e. Theory of Mind (ToM), to build socially intelligent agents who are able to communicate and cooperate effectively to accomplish challenging tasks. With ToM, each agent is capable of inferring the mental states and intentions of others according to its (local) observation. Based on the inferred states, the agents decide\"when\"and with\"whom\"to share their intentions. With the information observed, inferred, and received, the agents decide their sub-goals and reach a consensus among the team. In the end, the low-level executors independently take primitive actions to accomplish the sub-goals. We demonstrate the idea in two typical target-oriented multi-agent tasks: cooperative navigation and multi-sensor target coverage. The experiments show that the proposed model not only outperforms the state-of-the-art methods on reward and communication efficiency, but also shows good generalization across different scales of the environment.",
        "tldr": "The proposed Theory of Mind model not only outperforms the state-of-the-art methods on reward and communication efficiency, but also shows good generalization across different scales of the environment.",
        "venue": "International Conference on Learning Representations",
        "referenceCount": 50,
        "citationCount": 33,
        "influentialCitationCount": 3
    },
    {
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "authors": [
            "Kanishk Gandhi",
            "Jan-Philipp Franken",
            "Tobias Gerstenberg",
            "Noah D. Goodman"
        ],
        "abstract": "As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",
        "tldr": "This work presents a novel framework for procedurally generating evaluations with LLMs by populating causal templates and creates a new social reasoning benchmark (BigToM) for LLMs which is found that human participants rate the quality of this benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations.",
        "venue": "Neural Information Processing Systems",
        "referenceCount": 50,
        "citationCount": 14,
        "influentialCitationCount": 3
    },
    {
        "title": "Emergent Abilities of Large Language Models",
        "authors": [
            "Jason Wei",
            "Yi Tay",
            "Rishi Bommasani",
            "Colin Raffel",
            "Barret Zoph",
            "Sebastian Borgeaud",
            "Dani Yogatama",
            "Maarten Bosma",
            "Denny Zhou",
            "Donald Metzler",
            "E. Chi",
            "Tatsunori Hashimoto",
            "O. Vinyals",
            "P. Liang",
            "J. Dean",
            "W. Fedus"
        ],
        "abstract": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
        "tldr": "This paper discusses an unpredictable phenomenon that is referred to as emergent abilities of large language models, an ability to be emergent if it is not present in smaller models but is present in larger models.",
        "venue": "Trans. Mach. Learn. Res.",
        "referenceCount": 107,
        "citationCount": 1163,
        "influentialCitationCount": 83
    },
    {
        "title": "Language Model Cascades",
        "authors": [
            "David Dohan",
            "Winnie Xu",
            "Aitor Lewkowycz",
            "Jacob Austin",
            "David Bieber",
            "Raphael Gontijo Lopes",
            "Yuhuai Wu",
            "H. Michalewski",
            "R. Saurous",
            "Jascha Narain Sohl-Dickstein",
            "Kevin Murphy",
            "Charles Sutton"
        ],
        "abstract": "Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.",
        "tldr": "This work formalizes several existing techniques from probabilistic programming, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use, and refers to the resulting programs as language model cascades.",
        "venue": "arXiv.org",
        "referenceCount": 27,
        "citationCount": 74,
        "influentialCitationCount": 7
    },
    {
        "title": "Reasoning with Language Model Prompting: A Survey",
        "authors": [
            "Shuofei Qiao",
            "Yixin Ou",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yunzhi Yao",
            "Shumin Deng",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen"
        ],
        "abstract": "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).",
        "tldr": "This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "referenceCount": 219,
        "citationCount": 125,
        "influentialCitationCount": 3
    },
    {
        "title": "A Survey of Deep Learning for Mathematical Reasoning",
        "authors": [
            "Pan Lu",
            "Liang Qiu",
            "Wenhao Yu",
            "S. Welleck",
            "Kai-Wei Chang"
        ],
        "abstract": "Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",
        "tldr": "This survey paper reviews the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions in this domain.",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "referenceCount": 233,
        "citationCount": 58,
        "influentialCitationCount": 3
    },
    {
        "title": "A Survey on In-context Learning",
        "authors": [
            "Qingxiu Dong",
            "Lei Li",
            "Damai Dai",
            "Ce Zheng",
            "Zhiyong Wu",
            "Baobao Chang",
            "Xu Sun",
            "Jingjing Xu",
            "Zhifang Sui"
        ],
        "abstract": "With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
        "tldr": "This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis.",
        "venue": "",
        "referenceCount": 127,
        "citationCount": 74,
        "influentialCitationCount": 4
    },
    {
        "title": "Natural Language Reasoning, A Survey",
        "authors": [
            "Fei Yu",
            "Hongbo Zhang",
            "Benyou Wang"
        ],
        "abstract": "This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.",
        "tldr": "This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically, and identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions innatural language reasoning research.",
        "venue": "",
        "referenceCount": 195,
        "citationCount": 26,
        "influentialCitationCount": 2
    },
    {
        "title": "Towards Better Chain-of-Thought Prompting Strategies: A Survey",
        "authors": [
            "Zihan Yu",
            "Liang He",
            "Zhen Wu",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "abstract": "Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its impressive strength when used as a prompting strategy for large language models (LLM). Recent years, the prominent effect of CoT prompting has attracted emerging research. However, there still lacks of a systematic summary about key factors of CoT prompting and comprehensive guide for prompts utilizing. For a deeper understanding about CoT prompting, we survey on a wide range of current research, presenting a systematic and comprehensive analysis on several factors that may influence the effect of CoT prompting, and introduce how to better apply it in different applications under these discussions. We further analyze the challenges and propose some future directions about CoT prompting. This survey could provide an overall reference on related research.",
        "tldr": "A systematic and comprehensive analysis on several factors that may influence the effect of CoT prompting are presented, and how to better apply it in different applications under these discussions are introduced.",
        "venue": "arXiv.org",
        "referenceCount": 113,
        "citationCount": 9,
        "influentialCitationCount": 1
    },
    {
        "title": "A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",
        "authors": [
            "Zheng Chu",
            "Jingchang Chen",
            "Qianglong Chen",
            "Weijiang Yu",
            "Tao He",
            "Haotian Wang",
            "Weihua Peng",
            "Ming Liu",
            "Bing Qin",
            "Ting Liu"
        ],
        "abstract": "Chain-of-thought reasoning, a cognitive process fundamental to human intelligence, has garnered significant attention in the realm of artificial intelligence and natural language processing. However, there still remains a lack of a comprehensive survey for this arena. To this end, we take the first step and present a thorough survey of this research field carefully and widely. We use X-of-Thought to refer to Chain-of-Thought in a broad sense. In detail, we systematically organize the current research according to the taxonomies of methods, including XoT construction, XoT structure variants, and enhanced XoT. Additionally, we describe XoT with frontier applications, covering planning, tool use, and distillation. Furthermore, we address challenges and discuss some future directions, including faithfulness, multi-modal, and theory. We hope this survey serves as a valuable resource for researchers seeking to innovate within the domain of chain-of-thought reasoning.",
        "tldr": "A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.",
        "venue": "arXiv.org",
        "referenceCount": 210,
        "citationCount": 24,
        "influentialCitationCount": 2
    },
    {
        "title": "NovaCOMET: Open Commonsense Foundation Models with Symbolic Knowledge Distillation",
        "authors": [
            "Peter West",
            "R. L. Bras",
            "Taylor Sorensen",
            "Bill Yuchen Lin",
            "Liwei Jiang",
            "Ximing Lu",
            "Khyathi Raghavi Chandu",
            "Jack Hessel",
            "Ashutosh Baheti",
            "Chandra Bhagavatula",
            "Yejin Choi"
        ],
        "abstract": "We present NovaCOMET, an open commonsense knowledge model, that combines the best aspects of knowledge and general task models. Compared to previous knowledge models, NovaCOMET allows open-format relations enabling direct application to reasoning tasks; compared to general task models like Flan-T5, it explicitly centers knowledge, enabling superior performance for commonsense reasoning. NovaCOMET leverages the knowledge of opaque proprietary models to create an open knowledge pipeline. First, knowledge is symbolically distilled into NovATOMIC, a publicly-released discrete knowledge graph which can be audited, critiqued, and filtered. Next, we train NovaCOMET on NovATOMIC by fine-tuning an open-source pretrained model. NovaCOMET uses an open-format training objective, replacing the fixed relation sets of past knowledge models, enabling arbitrary structures within the data to serve as inputs or outputs. The resulting generation model, optionally augmented with human annotation, matches or exceeds comparable open task models like Flan-T5 on a range of commonsense generation tasks. NovaCOMET serves as a counterexample to the contemporary focus on instruction tuning only, demonstrating a distinct advantage to explicitly modeling commonsense knowledge as well.",
        "tldr": "NovaCOMET serves as a counterexample to the contemporary focus on instruction tuning only, demonstrating a distinct advantage to explicitly modeling commonsense knowledge as well.",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "referenceCount": 171,
        "citationCount": 4,
        "influentialCitationCount": 0
    },
    {
        "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
        "authors": [
            "Wenlong Huang",
            "P. Abbeel",
            "Deepak Pathak",
            "Igor Mordatch"
        ],
        "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner",
        "tldr": "This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions.",
        "venue": "International Conference on Machine Learning",
        "referenceCount": 54,
        "citationCount": 491,
        "influentialCitationCount": 48
    }
]