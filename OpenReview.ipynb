{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def fetch_and_categorize_papers(venue_id):\n",
    "    url = f\"https://api2.openreview.net/notes?content.venueid={venue_id}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Initialize dictionaries to hold categorized papers\n",
    "    papers_by_type = {'Oral': [], 'Spotlight': [], 'Poster': []}\n",
    "    \n",
    "    # Iterate over all papers and categorize them\n",
    "    if 'notes' in data:\n",
    "        for note in data['notes']:\n",
    "            venue_info = note['content'].get('venue', {})\n",
    "            if 'value' in venue_info:\n",
    "                venue_value = venue_info['value']\n",
    "                if 'oral' in venue_value.lower():\n",
    "                    papers_by_type['Oral'].append(note)\n",
    "                elif 'spotlight' in venue_value.lower():\n",
    "                    papers_by_type['Spotlight'].append(note)\n",
    "                elif 'poster' in venue_value.lower():\n",
    "                    papers_by_type['Poster'].append(note)\n",
    "    \n",
    "    return papers_by_type\n",
    "\n",
    "\n",
    "def filter_paper_by_topic(venue_id, topic, papers_by_type=None):\n",
    "    if not papers_by_type:\n",
    "        fetch_and_categorize_papers(venue_id)\n",
    "    # Print the results to verify\n",
    "    # for paper_type, notes in papers_by_type.items():\n",
    "    #     print(f\"\\n{paper_type} Papers:\")\n",
    "    #     if notes:\n",
    "    #         for note in notes[:5]:  # Limiting to first 5 papers for brevity\n",
    "    #             title = note.get('content', {}).get('title', 'No title available')\n",
    "    #             authors = \", \".join(note.get('content', {}).get('authors', []))\n",
    "    #             abstract = note.get('content', {}).get('abstract', 'No abstract available')\n",
    "    #             tldr = note.get('content', {}).get('TLDR', 'No TL;DR available')\n",
    "    #             keywords = note.get('content', {}).get('keywords', 'No keywords available')\n",
    "    #             link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "    #             print(f\"Title: {title}\")\n",
    "    #             print(f\"Authors: {authors}\")\n",
    "    #             print(f\"Abstract: {abstract}\")\n",
    "    #             print(f\"TL;DR: {tldr}\")\n",
    "    #             print(f\"Keywords: {keywords}\")\n",
    "    #             print(f\"Link: {link}\")\n",
    "    #             print(\"---\")\n",
    "    #     else:\n",
    "    #         print(\"No papers found.\")\n",
    "\n",
    "    for paper_type, notes in papers_by_type.items():\n",
    "        print(f\"\\n{paper_type} Papers:\")\n",
    "        if notes:\n",
    "            for note in notes: \n",
    "                title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "                authors = \", \".join(note.get('content', {}).get('authors', []).get('value'))\n",
    "                abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "                tldr = note.get('content', {}).get('TLDR', '')\n",
    "                if tldr:\n",
    "                    tldr = tldr['value']\n",
    "                else:\n",
    "                    tldr = 'No TL;DR available'\n",
    "\n",
    "                keywords = note.get('content', {}).get('keywords', 'No keywords available').get('value')\n",
    "                keywords = '; '.join(keywords)\n",
    "\n",
    "                if (topic in tldr.lower()) or (topic in keywords.lower()) or (topic in abstract.lower()) or (topic in title.lower()):\n",
    "                    # print(f\"Reasoning Paper\")\n",
    "\n",
    "                    # link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "                    print(f\"Title: {title}\")\n",
    "                    print(f\"Authors: {authors}\")\n",
    "                    # print(f\"Abstract: {abstract}\")\n",
    "                    print(f\"TL;DR: {tldr}\")\n",
    "                    print(f\"Keywords: {keywords}\")\n",
    "                    # print(f\"Link: {link}\")\n",
    "                    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "venue = 'ICLR' # NeurIPS\n",
    "year = 2024 # 2023\n",
    "venue_id = f\"{venue}.cc/{year}/Conference\"\n",
    "# venue_id = \"NeurIPS.cc/2023/Conference\"\n",
    "\n",
    "papers_by_type = fetch_and_categorize_papers(venue_id)\n",
    "\n",
    "with open(f'res/{venue}{year}.json', 'w+') as f:\n",
    "    json.dump(papers_by_type, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oral Papers:\n",
      "Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement\n",
      "Authors: Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang Ren\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: language model; natural language processing; inductive reasoning\n",
      "---\n",
      "Title: MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts\n",
      "Authors: Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao\n",
      "TL;DR: We introduce MathVista, a novel benchmark for evaluating mathematical reasoning capabilities within visual contexts, and conduct extensive experiments on 11 foundation models.\n",
      "Keywords: large language models; large multimodal models; mathematical reasoning; vision-language reasoning; foundation models and their evaluations\n",
      "---\n",
      "Title: SWE-bench: Can Language Models Resolve Real-world Github Issues?\n",
      "Authors: Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik R Narasimhan\n",
      "TL;DR: A novel benchmark for evaluating language models that introduces software engineering as a task.\n",
      "Keywords: Language models; Natural language processing; Software engineering\n",
      "---\n",
      "Title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n",
      "Authors: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi\n",
      "TL;DR: We introduce Self-RAG, a new training and inference framework to enable an LM learn to retrieve, generate and critique.\n",
      "Keywords: Retrieval-augmented Generation; Language Models; Retrieval-augmented LMs; Factuality\n",
      "---\n",
      "\n",
      "Spotlight Papers:\n",
      "Title: The Consensus Game: Language Model Generation via Equilibrium Search\n",
      "Authors: Athul Paul Jacob, Yikang Shen, Gabriele Farina, Jacob Andreas\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: language models; decoding; planning; game theory\n",
      "---\n",
      "Title: MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning\n",
      "Authors: Zayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett\n",
      "TL;DR: We introduce MuSR, a new dataset for testing LLMs' abilities to do complex, structured reasoning based on generated narratives.\n",
      "Keywords: Large Language Models; Chain-of-Thought; Textual Reasoning\n",
      "---\n",
      "Title: In-Context Pretraining: Language Modeling Beyond Document Boundaries\n",
      "Authors: Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Wen-tau Yih, Mike Lewis\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models\n",
      "---\n",
      "Title: Tool-Augmented Reward Modeling\n",
      "Authors: Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, Hua Wu\n",
      "TL;DR: This paper introduces tool-augmented reward models for reinforcement learning from human feedback (RLHF), improving precision and interpretability and contributing a comprehensive dataset from seven diverse tool APIs to advance the field.\n",
      "Keywords: Reward Model; Large Language Model; Tool Learning; Augmented Language Model\n",
      "---\n",
      "Title: Spatially-Aware Transformers for Embodied Agents\n",
      "Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn\n",
      "TL;DR: We propose a transformer-based episodic memory model, the Spatially-Aware Episodic Transformer, that incorporates both temporal and spatial dimensions to improve memory utilization and downstream task accuracy.\n",
      "Keywords: Episodic Memory; Spatial Inference; Prediction; Generation; Reinforcement Learning\n",
      "---\n",
      "Title: Grounding Language Plans in Demonstrations Through Counterfactual Perturbations\n",
      "Authors: Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Grounding LLM; Learning Mode Abstractions for Manipulation; Learning from Demonstration; Robotics; Task and Motion Planning\n",
      "---\n",
      "Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents\n",
      "Authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap\n",
      "TL;DR: SOTOPIA is a novel, challenging, and interactive benchmark that could serve as the perfect test-bed and potential incubator for social intelligence.\n",
      "Keywords: Social; Interaction; Agent; Social intelligence; Large Language Models; Evaluation; Theory of Mind\n",
      "---\n",
      "Title: EQA-MX: Embodied Question Answering using Multimodal Expression\n",
      "Authors: Md Mofijul Islam, Alexi Gladstone, Riashat Islam, Tariq Iqbal\n",
      "TL;DR: We present EQA-MX, a dataset and benchmark tasks for embodied multimodal QA, and VQ-Fusion model, enhancing visual-language alignment that outperforming existing models by 13%.\n",
      "Keywords: multimodal representation learning; visual-language models; embodied question answering\n",
      "---\n",
      "Title: On the Markov Property of Neural Algorithmic Reasoning: Analyses and Methods\n",
      "Authors: Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji\n",
      "TL;DR: This work reveals the importance of aligning model design with the Markov nature in neural algorithmic reasoning tasks.\n",
      "Keywords: Neural Algorithmic Reasoning\n",
      "---\n",
      "Title: DSPy: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines\n",
      "Authors: Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan A, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts\n",
      "TL;DR: We propose a programming model that unifies LM prompting, finetuning, and augmentation techniques. We evaluate simple strategies to bootstrap and optimize complex and multi-stage reasoning chains, establishing strong results with small and large LMs.\n",
      "Keywords: programming models; prompting techniques; in-context learning; few-shot learning; chain of thought; multi-hop reasoning; language agents\n",
      "---\n",
      "Title: Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics\n",
      "Authors: Christian Gumbsch, Noor Sajid, Georg Martius, Martin V. Butz\n",
      "TL;DR: We propose an algorithm to learn a hierarchy of world models from sparse latent state changes for explainable, long-horizon planning.\n",
      "Keywords: world models; temporal abstraction; hierarchical learning; model-based reinforcement learning; hierarchical planning\n",
      "---\n",
      "\n",
      "Poster Papers:\n",
      "Title: Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models\n",
      "Authors: Sijia Chen, Baochun Li, Di Niu\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Prompt Engineering; Boosting Mechanism;\n",
      "---\n",
      "Title: Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation\n",
      "Authors: Niels Mündler, Jingxuan He, Slobodan Jenko, Martin Vechev\n",
      "TL;DR: We present a comprehensive analysis showing that state-of-the-art LLMs frequently produce self-contradictory hallucinations. We then design prompting methods that effectively detect and mitigate self-contradictions.\n",
      "Keywords: language model; hallucination; trustworthy artificial intelligence; reasoning\n",
      "---\n",
      "Title: RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval\n",
      "Authors: Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D Manning\n",
      "TL;DR: RAPTOR improves LLM QA performance by constructing a hierarchical summarization tree for information retrieval, outperforming existing retrieval methods across various metrics and datasets.\n",
      "Keywords: Retrieval Augmented Language Models; Information Retrieval; summarization; QA\n",
      "---\n",
      "Title: Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources\n",
      "Authors: Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, Lidong Bing\n",
      "TL;DR: We present chain-of-knowledge, a novel framework that augments large language models dynamically by incorporating grounding information from heterogeneous sources.\n",
      "Keywords: large language model; knowledge grounding\n",
      "---\n",
      "Title: Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs\n",
      "Authors: Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sabharwal, Tushar Khot\n",
      "TL;DR: Assigning personas to LLMs can bring their deep-rooted biases to the surface, significantly diminishing their reasoning ability across domains.\n",
      "Keywords: Bias; Fairness; LLM; Reasoning; Persona; Safety\n",
      "---\n",
      "Title: Large Language Models as Automated Aligners for  benchmarking  Vision-Language Models\n",
      "Authors: Yuanfeng Ji, Chongjian GE, Weikai Kong, Enze Xie, Zhengying Liu, Zhenguo Li, Ping Luo\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: LLMs; VLMs; Benchmark\n",
      "---\n",
      "Title: Are Bert Family Good Instruction Followers?  A Study on Their Potential And Limitations\n",
      "Authors: yisheng xiao, Juntao Li, Zechen Sun, Zechang Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Min Zhang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Instruction tuning; Large language models; BERT family; Natural language generation\n",
      "---\n",
      "Title: Neural-Symbolic Recursive Machine for Systematic Generalization\n",
      "Authors: Qing Li, Yixin Zhu, Yitao Liang, Ying Nian Wu, Song-Chun Zhu, Siyuan Huang\n",
      "TL;DR: We present Neural-Symbolic Recursive Machine for systematic generalization, which achieves state-of-the-art performance on SCAN, PCFG, and HINT.\n",
      "Keywords: Neuro-symbolic AI; Systematic Generalization; Compositional Generalization\n",
      "---\n",
      "Title: Chain of Thought Empowers Transformers to Solve Inherently Serial Problems\n",
      "Authors: Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma\n",
      "TL;DR: We show both theoretically and empirically transformers with polynomial steps of CoT can simulate polysize circuits and thus are strictly more expressive than transformers without CoT.\n",
      "Keywords: Chain of thought; language modeling; circuit complexity; deep learning theory\n",
      "---\n",
      "Title: PlaSma: Procedural Knowledge Models for Language-based Planning and Re-Planning\n",
      "Authors: Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona Jacqueline Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi\n",
      "TL;DR: We propose a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language-based planning capabilities through knowledge distillation and an inference-time algorithm.\n",
      "Keywords: language-based planning; procedural/script knowledge; distillation; large language models; decoding-time algorithm\n",
      "---\n",
      "Title: FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n",
      "Authors: Daniel Y Fu, Hermann Kumbong, Eric Nguyen, Christopher Re\n",
      "TL;DR: We propose FlashFFTConv, a new system that optimizes the FFT convolution algorithm to speed up long convolutions and enable long-sequence applications.\n",
      "Keywords: convolutions; GPUs; hardware-efficient algorithms; long context; fast fourier transform; I/O awareness\n",
      "---\n",
      "Title: Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers\n",
      "Authors: Awni Altabaa, Taylor Whittington Webb, Jonathan D. Cohen, John Lafferty\n",
      "TL;DR: An extension of Transformers is proposed for explicit representation of relational information.\n",
      "Keywords: relational representation learning; attention; transformers; sequence models; abstract representations\n",
      "---\n",
      "Title: Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning\n",
      "Authors: Ahmed Abdulaal, adamos hadjivasiliou, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C. Castro, Daniel C. Alexander\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Causal Reasoning; Causal Discovery; Structural Causal Models; Large Language Models\n",
      "---\n",
      "Title: BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models\n",
      "Authors: Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li\n",
      "TL;DR: We proposed the first backdoor attack against LLMs with COT prompting that does not require access to the training set or model parameters.\n",
      "Keywords: large language model; chain-of-thought; backdoor attack; reasoning task\n",
      "---\n",
      "Title: Llemma: An Open Language Model for Mathematics\n",
      "Authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: reasoning; language models; pretraining\n",
      "---\n",
      "Title: Can Large Language Models Infer Causation from Correlation?\n",
      "Authors: Zhijing Jin, Jiarui Liu, Zhiheng LYU, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, Bernhard Schölkopf\n",
      "TL;DR: We propose Corr2Cause, the first dataset to infer causation from correlation by pure reasoning, and test 17 LLMs' performance on it.\n",
      "Keywords: Large Language Models; Natural Language Inference; Causal Reasoning; Correlation-to-Causation Inference; Benchmark Dataset; Causal Discovery\n",
      "---\n",
      "Title: REFACTOR: Learning to Extract Theorems from Proofs\n",
      "Authors: Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Baker Grosse\n",
      "TL;DR: We extract useful mathematical theorems using graph neural networks, evaluating on several downstream tasks to demonstrate their great utility.\n",
      "Keywords: theorem extraction; mathematical reasoning; theorem proving\n",
      "---\n",
      "Title: Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization\n",
      "Authors: Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy, Kilian Q Weinberger, Yuhuai Wu\n",
      "TL;DR: We show that automatically formalizing and verifying LLM generated quantitative reasoning solutions consistently outperforms vanilla majority voting.\n",
      "Keywords: mathematical reasoning; autoformalization; automated theorem proving; quantitative reasoning\n",
      "---\n",
      "Title: Understanding Catastrophic Forgetting in Language Models via Implicit Inference\n",
      "Authors: Suhas Kotha, Jacob Mitchell Springer, Aditi Raghunathan\n",
      "TL;DR: Fine-tuning may be understood as changing how a model infers the task of the prompt, and this allows us to recover the pretrained capabilities of language models through conjugate prompting.\n",
      "Keywords: implicit inference in language models; fine-tuning; catastrophic forgetting\n",
      "---\n",
      "Title: Deep SE(3)-Equivariant Geometric Reasoning for Precise Placement Tasks\n",
      "Authors: Ben Eisner, Yi Yang, Todor Davchev, Mel Vecerik, Jonathan Scholz, David Held\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Learning from Demonstration; Manipulation; 3D Learning; SE(3) Equivariance\n",
      "---\n",
      "Title: Lemur: Integrating Large Language Models in Automated Program Verification\n",
      "Authors: Haoze Wu, Clark Barrett, Nina Narodytska\n",
      "TL;DR: We present a general methodology for combining LLMs and formal verifiers for automated program verification.\n",
      "Keywords: Large Language Models; Formal verification\n",
      "---\n",
      "Title: CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models\n",
      "Authors: Sreyan Ghosh, Ashish Seth, Sonal Kumar, Utkarsh Tyagi, Chandra Kiran Reddy Evuru, Ramaneswaran S, S Sakshi, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: audio; audio-language; compositional reasoning\n",
      "---\n",
      "Title: COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits\n",
      "Authors: Mintong Kang, Nezihe Merve Gürel, Linyi Li, Bo Li\n",
      "TL;DR: We propose a certifiably robust learning-reasoning pipeline for conformal prediction.\n",
      "Keywords: conformal prediction; adversarial robustness; probabilistic circuits\n",
      "---\n",
      "Title: Contrastive Difference Predictive Coding\n",
      "Authors: Chongyi Zheng, Ruslan Salakhutdinov, Benjamin Eysenbach\n",
      "TL;DR: a temporal difference version of contrastive predictive coding\n",
      "Keywords: contrastive learning; reinforcement learning; goal-reaching; goal-conditioned RL; temporal difference\n",
      "---\n",
      "Title: Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning\n",
      "Authors: Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li\n",
      "TL;DR: We propose early-stopping self-consistency (ESC), a simple and scalable process which is capable of greatly reducing the cost of SC while maintaining performance.\n",
      "Keywords: Self-consistency; Chain-of-Thoughts; Multi-Step Reasoning; Large Language Models\n",
      "---\n",
      "Title: LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation\n",
      "Authors: Suhyeon Lee, Won Jun Kim, Jinho Chang, Jong Chul Ye\n",
      "TL;DR: We present a state-of-the-art multimodal LLM for chest X-ray understanding and generation, developed using a method that builds upon the transformer+VQGAN architecture and adapts it for instruction-finetuning of an LLM pretrained only on text.\n",
      "Keywords: large language model; multimodal; medical imaging; chest X-ray; bidirectional; instruction-tuning; vision-question answering\n",
      "---\n",
      "Title: Making Retrieval-Augmented Language Models Robust to Irrelevant Context\n",
      "Authors: Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan Berant\n",
      "TL;DR: We present a thorough analysis of cases where retrieval augmentation hurts performance of large language models, and propose methods that improve their robustness to irrelevant context, thereby increasing their overall performance.\n",
      "Keywords: Retrieval Augmented Language Models; Large Language Models; Robustness; Question Answering\n",
      "---\n",
      "Title: AgentBench: Evaluating LLMs as Agents\n",
      "Authors: Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang\n",
      "TL;DR: The first systematic benchmark for evaluating LLMs as Agents\n",
      "Keywords: Large language models; Autonomous agents; Reasoning; Evaluation; Benchmark\n",
      "---\n",
      "Title: ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search\n",
      "Authors: Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, Chao Zhang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Model; Tool Use; Tree Search; A* Search\n",
      "---\n",
      "Title: What Algorithms can Transformers Learn? A Study in Length Generalization\n",
      "Authors: Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Joshua M. Susskind, Samy Bengio, Preetum Nakkiran\n",
      "TL;DR: We show that length generalization of Transformer models trained from scratch strongly correlates with the simplicity of the true RASP-L program for the task.\n",
      "Keywords: length generalization; systematic generalization; understanding; transformer; scratchpad; LLM; algorithmic reasoning\n",
      "---\n",
      "Title: Understanding Expressivity of GNN in Rule Learning\n",
      "Authors: Haiquan Qiu, Yongqi Zhang, Yong Li, quanming yao\n",
      "TL;DR: We analyze the expressivity of SOTA GNNs for KG reasoning by deducing the rule strcutures they can learn.\n",
      "Keywords: Graph Neural Networks; KG reasoning; Link prediction; Rule learning; Expressivity\n",
      "---\n",
      "Title: COLLIE: Systematic Construction of Constrained Text Generation Tasks\n",
      "Authors: Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, Karthik R Narasimhan\n",
      "TL;DR: We propose a new framework for the systematic construction of challenging instances for constrained text generation.\n",
      "Keywords: constrained text generation; large language models; compositional benchmark\n",
      "---\n",
      "Title: GENOME: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules\n",
      "Authors: Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, Chuang Gan\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large language models; Neuro-symbolic Visual Reasoning\n",
      "---\n",
      "Title: Generating Pragmatic Examples to Train Neural Program Synthesizers\n",
      "Authors: Saujas Vaduguru, Daniel Fried, Yewen Pu\n",
      "TL;DR: Pragmatic program synthesis in a realistic program space without human supervision in training\n",
      "Keywords: program synthesis; pragmatics; self-play\n",
      "---\n",
      "Title: CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding\n",
      "Authors: Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan\n",
      "TL;DR: Our proposed CoVLM improves compositional reasoning ability of VLM through dynamically communicating between vision and language, achieving the SOTA on both compositional reasoning task and traditional VL tasks.\n",
      "Keywords: vision-language model; compositionality\n",
      "---\n",
      "Title: Building Cooperative Embodied Agents Modularly with Large Language Models\n",
      "Authors: Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, Chuang Gan\n",
      "TL;DR: We present CoELA, a modular framework integrating LLMs to address the challenging multi-agent embodied cooperation problem with decentralized control, costly communication, and long-horizon multi-objective tasks.\n",
      "Keywords: Large Language Models; Embodied Intelligence; Multi-Agent Cooperation; Human-AI Interaction; Communication\n",
      "---\n",
      "Title: Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models\n",
      "Authors: Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: visual question answering; zero-shot; large vision language models; visual reasoning; underspecification; grounding language to vision\n",
      "---\n",
      "Title: Let Models Speak Ciphers: Multiagent Debate through Embeddings\n",
      "Authors: Chau Pham, Boyi Liu, Yingxiang Yang, Zhengyu Chen, Tianyi Liu, Jianbo Yuan, Bryan A. Plummer, Zhaoran Wang, Hongxia Yang\n",
      "TL;DR: We present a novel communication approach for Large Language Models (LLMs) by removing the token sampling step from LLMs and enabling them to convey their beliefs across the vocabulary through the expectation of raw transformer output embeddings.\n",
      "Keywords: multiagent debate; large language models; inter-model communication; embedding representation\n",
      "---\n",
      "Title: Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words\n",
      "Authors: Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos\n",
      "TL;DR: ChannelViT facilitates robust representation learning across different input channels.\n",
      "Keywords: vision transformer; representation learning; hyper spectral imaging\n",
      "---\n",
      "Title: LLM Augmented LLMs: Expanding Capabilities through Composition\n",
      "Authors: Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Model Composition; Knowledge Augmentation\n",
      "---\n",
      "Title: When can transformers reason with abstract symbols?\n",
      "Authors: Enric Boix-Adserà, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua M. Susskind\n",
      "TL;DR: Transformers learn to reason relationally given enough data, and we improve data efficiency with theory-inspired architecture modifications.\n",
      "Keywords: transformers; language models; reasoning; theoretical analysis; variable binding\n",
      "---\n",
      "Title: Towards Generative Abstract Reasoning: Completing Raven’s Progressive Matrix via Rule Abstraction and Selection\n",
      "Authors: Fan Shi, Bin Li, Xiangyang Xue\n",
      "TL;DR: This paper proposes a novel deep latent variable model to solve generative RPM problems through rule abstraction and selection.\n",
      "Keywords: Deep Latent Variable Models; Generative Models; Raven’s Progressive Matrix; Abstract Visual Reasoning\n",
      "---\n",
      "Title: Think before you speak: Training Language Models With Pause Tokens\n",
      "Authors: Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan\n",
      "TL;DR: We explore delaying model's prediction of next token, by appending (learnable) pause tokens to allow increased inference-time computations .\n",
      "Keywords: LLM training and inference; Downstream finetuning\n",
      "---\n",
      "Title: Talk like a Graph: Encoding Graphs for Large Language Models\n",
      "Authors: Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Graph problems; large language models; encoding graphs; generative models\n",
      "---\n",
      "Title: OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text\n",
      "Authors: Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, Jimmy Ba\n",
      "TL;DR: We open-source a large-scale mathematical dataset extracted from the web.\n",
      "Keywords: web-scale dataset; natural language processing; large language model; reasoning; AI for math\n",
      "---\n",
      "Title: Vision-by-Language for Training-Free Compositional Image Retrieval\n",
      "Authors: Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata\n",
      "TL;DR: A simple method using off-the-shelf foundation models for Composed Image Retrieval without any training\n",
      "Keywords: Vision-Language Models; Large Language Models\n",
      "---\n",
      "Title: Chain-of-Experts: When LLMs Meet Complex Operations Research Problems\n",
      "Authors: Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, Gang Chen\n",
      "TL;DR: This paper introduces Chain-of-Experts (CoE), a multi-agent LLM framework that boosts reasoning in complex operation research problems by integrating domain-specific agents under a conductor's guidance and reflection mechanism.\n",
      "Keywords: Large Language Model; Operations Research\n",
      "---\n",
      "Title: GAIA: a benchmark for General AI Assistants\n",
      "Authors: Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, Thomas Scialom\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Large Language Models; Benchmark\n",
      "---\n",
      "Title: SALMONN: Towards Generic Hearing Abilities for Large Language Models\n",
      "Authors: Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, Chao Zhang\n",
      "TL;DR: SALMONN: Towards Generic Hearing Abilities for Large Language Models\n",
      "Keywords: Multimodal large language models; speech and audio processing; music processing\n",
      "---\n",
      "Title: Magnushammer: A Transformer-Based Approach to Premise Selection\n",
      "Authors: Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q. Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, Yuhuai Wu\n",
      "TL;DR: Contrastively trained transformers outperform state-of-the-art symbolic methods for premise selection, a challenging reasoning task of selecting relevant facts for proving new theorems in formal mathematics.\n",
      "Keywords: transformers; interactive theorem proving; automated reasoning; contrastive learning; premise selection\n",
      "---\n",
      "Title: Learning to Compose: Improving Object Centric Learning by Injecting Compositionality\n",
      "Authors: Whie Jung, Jaehoon Yoo, Sungjin Ahn, Seunghoon Hong\n",
      "TL;DR: We propose a novel objective that explicitly encourages compositionality of the representations.\n",
      "Keywords: Object-Centric learning; Compositionality\n",
      "---\n",
      "Title: Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning\n",
      "Authors: LINHAO LUO, Yuan-Fang Li, Reza Haf, Shirui Pan\n",
      "TL;DR: we propose a novel method called Reasoning on Graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning.\n",
      "Keywords: large language models; knowledge graphs; reasoning\n",
      "---\n",
      "Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World\n",
      "Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun Zhu, Yizhou Wang\n",
      "TL;DR: No TL;DR available\n",
      "Keywords: Few-shot learning; Visual reasoning; Open world learning\n",
      "---\n",
      "Title: The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models\n",
      "Authors: Raphaël Avalos, Florent Delgrange, Ann Nowe, Guillermo Perez, Diederik M Roijers\n",
      "TL;DR: Wasserstein Belief Updater is an RNN free RL algorithm for POMDPs that learns a representation of the history via an approximation of the belief update in a reliable latent space model, providing theoretical guarantees for learning the optimal value.\n",
      "Keywords: pomdp; guarantees; representation learning; reinforcement learning\n",
      "---\n",
      "Title: Turning large language models into cognitive models\n",
      "Authors: Marcel Binz, Eric Schulz\n",
      "TL;DR: We finetune a large language model on data from psychological experiments and find that doing so produces models that are more aligned with human decision-making.\n",
      "Keywords: cognitive modeling; large language models; neural networks; cognitive psychology; decision-making\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# with open(f'res/{venue}{year}.json', 'w+') as f:\n",
    "#     papers_by_type = json.load(f)\n",
    "\n",
    "filter_paper_by_topic(venue_id, 'reasoning', papers_by_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
