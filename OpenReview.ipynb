{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OpenReview papers by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os \n",
    "import json \n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def fetch_and_categorize_papers(venue, year):\n",
    "    offset = 0\n",
    "    limit = 1000  # Adjust as needed based on API's maximum allowed limit per request\n",
    "    all_papers = []\n",
    "    \n",
    "    venue_id = f\"{venue}.cc/{year}/Conference\"\n",
    "\n",
    "    if os.path.exists(f'res/{venue}{year}.json'):\n",
    "        return json.load(open(f'res/{venue}{year}.json', 'r'))\n",
    "\n",
    "    while True:\n",
    "        url = f\"https://api2.openreview.net/notes?content.venueid={venue_id}&offset={offset}&limit={limit}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            data = response.json()\n",
    "            if 'notes' not in data or not data['notes']:\n",
    "                break  # Exit the loop if no more papers are available\n",
    "            all_papers.extend(data['notes'])\n",
    "            offset += limit  # Prepare offset for the next page of results\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to retrieve data: {e}\")\n",
    "            return {}\n",
    "\n",
    "    # Initialize dictionaries to hold categorized papers\n",
    "    papers_by_type = {'Oral': [], 'Spotlight': [], 'Poster': []}\n",
    "    \n",
    "    # Categorize the papers\n",
    "    for note in all_papers:\n",
    "        venue_info = note['content'].get('venue', {})\n",
    "        if 'value' in venue_info:\n",
    "            venue_value = venue_info['value']\n",
    "            if 'oral' in venue_value.lower():\n",
    "                papers_by_type['Oral'].append(note)\n",
    "            elif 'spotlight' in venue_value.lower():\n",
    "                papers_by_type['Spotlight'].append(note)\n",
    "            elif 'poster' in venue_value.lower():\n",
    "                papers_by_type['Poster'].append(note)\n",
    "\n",
    "    return papers_by_type\n",
    "\n",
    "\n",
    "def filter_paper_by_topic(venue, year, topic, papers_by_type=None):\n",
    "    venue_id = f\"{venue}.cc/{year}/Conference\"\n",
    "\n",
    "    if not papers_by_type:\n",
    "        fetch_and_categorize_papers(venue_id)\n",
    "    # Print the results to verify\n",
    "    # for paper_type, notes in papers_by_type.items():\n",
    "    #     print(f\"\\n{paper_type} Papers:\")\n",
    "    #     if notes:\n",
    "    #         for note in notes[:5]:  # Limiting to first 5 papers for brevity\n",
    "    #             title = note.get('content', {}).get('title', 'No title available')\n",
    "    #             authors = \", \".join(note.get('content', {}).get('authors', []))\n",
    "    #             abstract = note.get('content', {}).get('abstract', 'No abstract available')\n",
    "    #             tldr = note.get('content', {}).get('TLDR', 'No TL;DR available')\n",
    "    #             keywords = note.get('content', {}).get('keywords', 'No keywords available')\n",
    "    #             link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "    #             print(f\"Title: {title}\")\n",
    "    #             print(f\"Authors: {authors}\")\n",
    "    #             print(f\"Abstract: {abstract}\")\n",
    "    #             print(f\"TL;DR: {tldr}\")\n",
    "    #             print(f\"Keywords: {keywords}\")\n",
    "    #             print(f\"Link: {link}\")\n",
    "    #             print(\"---\")\n",
    "    #     else:\n",
    "    #         print(\"No papers found.\")\n",
    "\n",
    "    for paper_type, notes in papers_by_type.items():\n",
    "        print(f\"\\n{paper_type} Papers:\")\n",
    "        if notes:\n",
    "            for note in notes: \n",
    "                title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "                authors = \", \".join(note.get('content', {}).get('authors', []).get('value'))\n",
    "                abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "                tldr = note.get('content', {}).get('TLDR', '')\n",
    "                if tldr:\n",
    "                    tldr = tldr['value']\n",
    "                else:\n",
    "                    tldr = 'No TL;DR available'\n",
    "\n",
    "                keywords = note.get('content', {}).get('keywords', 'No keywords available').get('value')\n",
    "                keywords = '; '.join(keywords)\n",
    "\n",
    "                if (topic in tldr.lower()) or (topic in keywords.lower()) or (topic in abstract.lower()) or (topic in title.lower()):\n",
    "                    # print(f\"Reasoning Paper\")\n",
    "                    \n",
    "                    link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "                    print(f\"Title: {title}\")\n",
    "                    print(f\"Authors: {authors}\")\n",
    "                    # print(f\"Abstract: {abstract}\")\n",
    "                    print(f\"TL;DR: {tldr}\")\n",
    "                    print(f\"Keywords: {keywords}\")\n",
    "                    print(f\"Link: {link}\")\n",
    "                    print(\"---\")\n",
    "                    \n",
    "\n",
    "def json_to_pandas(papers):\n",
    "    df = pd.DataFrame()\n",
    "    for paper_type, notes in papers.items():\n",
    "        if notes:\n",
    "            for note in tqdm(notes, desc=f'Processing {paper_type} papers...'): \n",
    "                title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "                authors = note.get('content', {}).get('authors', 'None')\n",
    "                if authors != None:\n",
    "                    authors = \", \".join(authors)\n",
    "                abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "                tldr = note.get('content', {}).get('TLDR', '')\n",
    "                if tldr:\n",
    "                    tldr = tldr['value']\n",
    "                else:\n",
    "                    tldr = 'No TL;DR available'\n",
    "\n",
    "                keywords = note.get('content', {}).get('keywords', 'No keywords available')\n",
    "                if keywords != \"No keywords available\":\n",
    "                    keywords = keywords.get('value')\n",
    "                    keywords = '; '.join(keywords)\n",
    "                    \n",
    "                link = f\"https://openreview.net/forum?id={note['id']}\"\n",
    "\n",
    "                tmp = {\n",
    "                    'Title': title,\n",
    "                    'Tag': paper_type,\n",
    "                    'Keywords': keywords,\n",
    "                    'TLDR': tldr,\n",
    "                    'Authors': authors,\n",
    "                    'Abstract': abstract,\n",
    "                    'Link':link\n",
    "                }\n",
    "                df = df.append(tmp,ignore_index=True)\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Oral papers...: 100%|██████████| 67/67 [00:00<00:00, 231.62it/s]\n",
      "Processing Spotlight papers...: 100%|██████████| 378/378 [00:01<00:00, 244.12it/s]\n",
      "Processing Poster papers...: 100%|██████████| 2773/2773 [00:11<00:00, 235.38it/s]\n"
     ]
    }
   ],
   "source": [
    "venue = 'NeurIPS'\n",
    "year = 2023\n",
    "papers = fetch_and_categorize_papers(venue=venue, year=year)\n",
    "\n",
    "with open(f'res/{venue}{year}.json', 'w+') as f:\n",
    "    json.dump(papers, f)\n",
    "\n",
    "df = json_to_pandas(papers)\n",
    "df.to_csv(f'res/{venue}{year}.csv')\n",
    "\n",
    "# filter_paper_by_topic(venue, year, 'reason', papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Oral papers...: 100%|██████████| 86/86 [00:00<00:00, 259.66it/s]\n",
      "Processing Spotlight papers...: 100%|██████████| 367/367 [00:01<00:00, 233.97it/s]\n",
      "Processing Poster papers...: 100%|██████████| 1807/1807 [00:08<00:00, 216.35it/s]\n"
     ]
    }
   ],
   "source": [
    "venue = 'ICLR'\n",
    "year = 2024\n",
    "papers = fetch_and_categorize_papers(venue=venue, year=year)\n",
    "\n",
    "with open(f'res/{venue}{year}.json', 'w+') as f:\n",
    "    json.dump(papers, f)\n",
    "\n",
    "df = json_to_pandas(papers)\n",
    "df.to_csv(f'res/{venue}{year}.csv')\n",
    "\n",
    "# filter_paper_by_topic(venue, year, 'reason', papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue = 'ICML'\n",
    "year = 2024\n",
    "papers = fetch_and_categorize_papers(venue=venue, year=year)\n",
    "\n",
    "with open(f'res/{venue}{year}.json', 'w+') as f:\n",
    "    json.dump(papers, f)\n",
    "\n",
    "df = json_to_pandas(papers)\n",
    "df.to_csv(f'res/{venue}{year}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter paper with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oral Papers:\n",
      "\n",
      "Spotlight Papers:\n",
      "\n",
      "Poster Papers:\n",
      "[NeurIPS 2023] Tree of Thoughts: Deliberate Problem Solving with Large Language Models [[paper]](https://openreview.net/forum?id=5Xc1ecxO1h) \n",
      "\n",
      "[NeurIPS 2023] On the Planning Abilities of Large Language Models - A Critical Investigation [[paper]](https://openreview.net/forum?id=X6dEqXIsEW) \n",
      "\n",
      "[NeurIPS 2023] Can Language Models Solve Graph Problems in Natural Language? [[paper]](https://openreview.net/forum?id=UDqHhbqYJV) \n",
      "\n",
      "[NeurIPS 2023] SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks [[paper]](https://openreview.net/forum?id=Rzk3GP1HN7) \n",
      "\n",
      "[NeurIPS 2023] Parsel🐍: Algorithmic Reasoning with Language Models by Composing Decompositions [[paper]](https://openreview.net/forum?id=qd9qcbVAwQ) \n",
      "\n",
      "[NeurIPS 2023] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought [[paper]](https://openreview.net/forum?id=IL5zJqfxAa) \n",
      "\n",
      "[NeurIPS 2023] Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting [[paper]](https://openreview.net/forum?id=bzs4uPLXvi) \n",
      "\n",
      "[NeurIPS 2023] Egocentric Planning for Scalable Embodied Task Achievement [[paper]](https://openreview.net/forum?id=v0lkbp66Uw) \n",
      "\n",
      "[NeurIPS 2023] Grammar Prompting for Domain-Specific Language Generation with  Large Language Models [[paper]](https://openreview.net/forum?id=B4tkwuzeiY) \n",
      "\n",
      "[NeurIPS 2023] SatLM: Satisfiability-Aided Language Models Using Declarative Prompting [[paper]](https://openreview.net/forum?id=TqW5PL1Poi) \n",
      "\n",
      "[NeurIPS 2023] Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering [[paper]](https://openreview.net/forum?id=9WSxQZ9mG7) \n",
      "\n",
      "[NeurIPS 2023] AdaPlanner: Adaptive Planning from Feedback with Language Models [[paper]](https://openreview.net/forum?id=rnKgbKmelt) \n",
      "\n",
      "[NeurIPS 2023] Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning [[paper]](https://openreview.net/forum?id=zDbsSscmuj) \n",
      "\n",
      "[NeurIPS 2023] Can Language Models Teach? Teacher Explanations Improve Student Performance via Personalization [[paper]](https://openreview.net/forum?id=IacxcFpvWQ) \n",
      "\n",
      "[NeurIPS 2023] AVIS: Autonomous Visual Information Seeking with Large Language Model Agent [[paper]](https://openreview.net/forum?id=7EMphtUgCI) \n",
      "\n",
      "[NeurIPS 2023] Large Language Models as Commonsense Knowledge for Large-Scale Task Planning [[paper]](https://openreview.net/forum?id=Wjp1AYB8lH) \n",
      "\n",
      "[NeurIPS 2023] Evaluating Cognitive Maps and Planning in Large Language Models with CogEval [[paper]](https://openreview.net/forum?id=VtkGvGcGe3) \n",
      "\n",
      "[NeurIPS 2023] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face [[paper]](https://openreview.net/forum?id=yHdTscY6Ci) \n",
      "\n",
      "[NeurIPS 2023] Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents [[paper]](https://openreview.net/forum?id=KtvPdGb31Z) \n",
      "\n",
      "[NeurIPS 2023] Post Hoc Explanations of Language Models Can Improve Language Models [[paper]](https://openreview.net/forum?id=3H37XciUEv) \n",
      "\n",
      "[NeurIPS 2023] Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models [[paper]](https://openreview.net/forum?id=HtqnVSCj3q) \n",
      "\n",
      "[NeurIPS 2023] VPGTrans: Transfer Visual Prompt Generator across LLMs [[paper]](https://openreview.net/forum?id=716PvHoDct) \n",
      "\n",
      "[NeurIPS 2023] SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models [[paper]](https://openreview.net/forum?id=tfyr2zRVoK) \n",
      "\n",
      "[NeurIPS 2023] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models [[paper]](https://openreview.net/forum?id=Xu8aG5Q8M3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_papers(file_path, keywords, focus):\n",
    "    with open(file_path, 'r') as file:\n",
    "        papers_by_type = json.load(file)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for paper_type, notes in papers_by_type.items():\n",
    "        print(f\"\\n{paper_type} Papers:\")\n",
    "        if notes:\n",
    "            for note in notes: \n",
    "                # title = note.get('content', {}).get('title', 'No title available').get('value')\n",
    "                # authors = \", \".join(note.get('content', {}).get('authors', []).get('value'))\n",
    "                # abstract = note.get('content', {}).get('abstract', 'No abstract available').get('value')\n",
    "\n",
    "                # tldr = note.get('content', {}).get('TLDR', '')\n",
    "                # if tldr:\n",
    "                #     tldr = tldr['value']\n",
    "                # else:\n",
    "                #     tldr = 'No TL;DR available'\n",
    "\n",
    "                # keywords = note.get('content', {}).get('keywords', 'No keywords available').get('value')\n",
    "                # keywords = '; '.join(keywords)\n",
    "\n",
    "                content = note.get('content', {})\n",
    "                title = content.get('title', {}).get('value', '')\n",
    "                abstract = content.get('abstract', {}).get('value', '')\n",
    "                url = note.get('forum', '')\n",
    "                full_url = f\"https://openreview.net/forum?id={url}\"\n",
    "                \n",
    "                if any(keyword.lower() in title.lower() for keyword in keywords) or \\\n",
    "                any(keyword.lower() in abstract.lower() for keyword in keywords):\n",
    "                    if focus.lower() in title.lower() or focus.lower() in abstract.lower():\n",
    "                        formatted_entry = f\"[NeurIPS 2023] {title} [[paper]]({full_url}) \\n\"\n",
    "                        results.append(formatted_entry)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "file_path = 'res/NeurIPS2023.json'  # Replace with your actual file path\n",
    "keywords = ['planning', 'plan', 'plans']  # Add other related keywords as needed\n",
    "focus = 'LLM'\n",
    "\n",
    "extracted_papers = extract_papers(file_path, keywords, focus)\n",
    "for paper in extracted_papers:\n",
    "    print(paper)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
